{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_Seq2SeqImplementation__Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwPL0hIlGKoA"
      },
      "source": [
        "# <font color='red'>**Sequence to sequence implementation**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nYHE_1ck2az"
      },
      "source": [
        "**There will be some functions that start with the word \"grader\" ex: grader_check_encoder(), grader_check_attention(), grader_onestepdecoder() etc, you should not change those function definition.<br><br>Every Grader function has to return True.**\n",
        "\n",
        "**Note 1:**  There are many blogs on the attention mechanisum which might be misleading you,\n",
        " so do read the references completly and after that only please check the internet.\n",
        " The best things is to read the research papers and try to implement it on your own. \n",
        "\n",
        "**Note 2:** To complete this assignment, the reference that are mentioned will be enough.\n",
        "\n",
        "**Note 3:** If you are starting this assignment, you might have completed minimum of 20 assignment.\n",
        " If  you are still not able to implement this algorithm you might have rushed in the previous assignments \n",
        "with out learning much and didn't spend your time productively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyfZo8fmLOec"
      },
      "source": [
        "## Task -1: Simple Encoder and Decoder\n",
        "Implement simple Encoder-Decoder model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvNSZXNkkOkO"
      },
      "source": [
        "1. Download the **Italian** to **English** translation dataset from <a href=\"http://www.manythings.org/anki/ita-eng.zip\">here</a>\n",
        "\n",
        "2. You will find **ita.txt** file in that ZIP, \n",
        "you can read that data using python and preprocess that data this way only: \n",
        "<img src='https://i.imgur.com/z0j79Jf.png'>    \n",
        "    \n",
        "3. You have to implement a simple Encoder and Decoder architecture  \n",
        "\n",
        "4. Use BLEU score as metric to evaluate your model. You can use any loss function you need.\n",
        "\n",
        "5. You have to use Tensorboard to plot the Graph, Scores and histograms of gradients. \n",
        "\n",
        "6.  a. Check the reference notebook <br>\n",
        "    b. <a href=\"https://medium.com/analytics-vidhya/understand-sequence-to-sequence-models-in-a-more-intuitive-way-1d517d8795bb\">Resource 2</a>\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9jsu5D92jso",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2930d8f2-af80-49dd-aeb9-b2b40ef31155"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun May 30 14:35:01 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vtkp7s6R2Odu"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# import seaborn as sns\n",
        "import pandas as pd\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Reshape, Softmax, Dot, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import seaborn as sns"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k_AlAuKJqVA"
      },
      "source": [
        "<font color='blue'>**Load the data**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU80Ao-AGaob",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64112bbf-d1a7-4228-eb97-b60ca3a473ee"
      },
      "source": [
        "!wget http://www.manythings.org/anki/ita-eng.zip\n",
        "!unzip ita-eng.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-30 14:35:11--  http://www.manythings.org/anki/ita-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.21.55.222, 172.67.173.198, 2606:4700:3031::6815:37de, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.21.55.222|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7567445 (7.2M) [application/zip]\n",
            "Saving to: ‘ita-eng.zip’\n",
            "\n",
            "ita-eng.zip         100%[===================>]   7.22M  6.63MB/s    in 1.1s    \n",
            "\n",
            "2021-05-30 14:35:13 (6.63 MB/s) - ‘ita-eng.zip’ saved [7567445/7567445]\n",
            "\n",
            "Archive:  ita-eng.zip\n",
            "  inflating: ita.txt                 \n",
            "  inflating: _about.txt              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tQzC4qw1lOP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "62c4e2a9-d00c-4ee7-a87f-8051bf5e7faf"
      },
      "source": [
        "with open('ita.txt', 'r', encoding=\"utf8\") as f:\n",
        "    eng=[]\n",
        "    ita=[]\n",
        "    for i in f.readlines():\n",
        "        eng.append(i.split(\"\\t\")[0])\n",
        "        ita.append(i.split(\"\\t\")[1])\n",
        "data = pd.DataFrame(data=list(zip(eng, ita)), columns=['english','italian'])\n",
        "print(data.shape)\n",
        "data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(345244, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>english</th>\n",
              "      <th>italian</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Ciao!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Corri!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Corra!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Correte!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Who?</td>\n",
              "      <td>Chi?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  english   italian\n",
              "0     Hi.     Ciao!\n",
              "1    Run!    Corri!\n",
              "2    Run!    Corra!\n",
              "3    Run!  Correte!\n",
              "4    Who?      Chi?"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmGWTdRmKRph"
      },
      "source": [
        "<font color='blue'>**Preprocess data**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QqElB_nKZos",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "3568e318-ed8e-4289-e551-46487ffd9eff"
      },
      "source": [
        "def decontractions(phrase):\n",
        "    \"\"\"decontracted takes text and convert contractions into natural form.\n",
        "     ref: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python/47091490#47091490\"\"\"\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    phrase = re.sub(r\"won\\’t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "\n",
        "    phrase = re.sub(r\"n\\’t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\’re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\’s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\’d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\’ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\’t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\’ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\’m\", \" am\", phrase)\n",
        "\n",
        "    return phrase\n",
        "\n",
        "def preprocess(text):\n",
        "    text = decontractions(text)\n",
        "    text = re.sub('[^A-Za-z0-9 ]+', '', text)\n",
        "    return text\n",
        "\n",
        "def preprocess_ita(text):\n",
        "    text = text.lower()\n",
        "    text = decontractions(text)\n",
        "    text = re.sub('[$)\\?\"’.°!;\\'€%:,(/]', '', text)\n",
        "    text = re.sub('\\u200b', ' ', text)\n",
        "    text = re.sub('\\xa0', ' ', text)\n",
        "    text = re.sub('-', ' ', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "data['english'] = data['english'].apply(preprocess)\n",
        "data['italian'] = data['italian'].apply(preprocess_ita)\n",
        "data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>english</th>\n",
              "      <th>italian</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hi</td>\n",
              "      <td>ciao</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Run</td>\n",
              "      <td>corri</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Run</td>\n",
              "      <td>corra</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Run</td>\n",
              "      <td>correte</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Who</td>\n",
              "      <td>chi</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  english  italian\n",
              "0      Hi     ciao\n",
              "1     Run    corri\n",
              "2     Run    corra\n",
              "3     Run  correte\n",
              "4     Who      chi"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlzyjTy02oCd"
      },
      "source": [
        "ita_lengths = data['italian'].str.split().apply(len)\n",
        "eng_lengths = data['english'].str.split().apply(len)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8556LoG2vUM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "3073208d-fbd8-4ca7-abbe-76db1fb6ff04"
      },
      "source": [
        "data['italian_len'] = data['italian'].str.split().apply(len)\n",
        "data = data[data['italian_len'] < 20]\n",
        "\n",
        "data['english_len'] = data['english'].str.split().apply(len)\n",
        "data = data[data['english_len'] < 20]\n",
        "\n",
        "data['english_inp'] = '<start> ' + data['english'].astype(str)\n",
        "data['english_out'] = data['english'].astype(str) + ' <end>'\n",
        "\n",
        "data = data.drop(['english','italian_len','english_len'], axis=1)\n",
        "# only for the first sentance add a toke <end> so that we will have <end> in tokenizer\n",
        "data.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>italian</th>\n",
              "      <th>english_inp</th>\n",
              "      <th>english_out</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ciao</td>\n",
              "      <td>&lt;start&gt; Hi</td>\n",
              "      <td>Hi &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>corri</td>\n",
              "      <td>&lt;start&gt; Run</td>\n",
              "      <td>Run &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>corra</td>\n",
              "      <td>&lt;start&gt; Run</td>\n",
              "      <td>Run &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>correte</td>\n",
              "      <td>&lt;start&gt; Run</td>\n",
              "      <td>Run &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>chi</td>\n",
              "      <td>&lt;start&gt; Who</td>\n",
              "      <td>Who &lt;end&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   italian  english_inp english_out\n",
              "0     ciao   <start> Hi    Hi <end>\n",
              "1    corri  <start> Run   Run <end>\n",
              "2    corra  <start> Run   Run <end>\n",
              "3  correte  <start> Run   Run <end>\n",
              "4      chi  <start> Who   Who <end>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5J6FTOG2x1U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "9629c158-4f97-4952-fb2b-fb3814302c05"
      },
      "source": [
        "data.sample(10)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>italian</th>\n",
              "      <th>english_inp</th>\n",
              "      <th>english_out</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>122311</th>\n",
              "      <td>la radio non funziona</td>\n",
              "      <td>&lt;start&gt; The radio does not work</td>\n",
              "      <td>The radio does not work &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69375</th>\n",
              "      <td>tu non sei serio</td>\n",
              "      <td>&lt;start&gt; You are not serious</td>\n",
              "      <td>You are not serious &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110414</th>\n",
              "      <td>noi non possiamo trainare questa macchina</td>\n",
              "      <td>&lt;start&gt; We can not tow this car</td>\n",
              "      <td>We can not tow this car &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25459</th>\n",
              "      <td>ehi posso aiutare</td>\n",
              "      <td>&lt;start&gt; Hey can I help</td>\n",
              "      <td>Hey can I help &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255524</th>\n",
              "      <td>tom se ne occuperà</td>\n",
              "      <td>&lt;start&gt; Tom is going to take care of it</td>\n",
              "      <td>Tom is going to take care of it &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174528</th>\n",
              "      <td>voi dovete essere ottimisti</td>\n",
              "      <td>&lt;start&gt; You have to be optimistic</td>\n",
              "      <td>You have to be optimistic &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81155</th>\n",
              "      <td>faremmo meglio ad andare tom</td>\n",
              "      <td>&lt;start&gt; We would better go Tom</td>\n",
              "      <td>We would better go Tom &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12494</th>\n",
              "      <td>li avevo dimenticati</td>\n",
              "      <td>&lt;start&gt; I forgot them</td>\n",
              "      <td>I forgot them &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>219050</th>\n",
              "      <td>quando avete finito il lavoro</td>\n",
              "      <td>&lt;start&gt; When did you finish the work</td>\n",
              "      <td>When did you finish the work &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201622</th>\n",
              "      <td>tom è qui per proteggere mary</td>\n",
              "      <td>&lt;start&gt; Tom is here to protect Mary</td>\n",
              "      <td>Tom is here to protect Mary &lt;end&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          italian  ...                            english_out\n",
              "122311                      la radio non funziona  ...          The radio does not work <end>\n",
              "69375                            tu non sei serio  ...              You are not serious <end>\n",
              "110414  noi non possiamo trainare questa macchina  ...          We can not tow this car <end>\n",
              "25459                           ehi posso aiutare  ...                   Hey can I help <end>\n",
              "255524                         tom se ne occuperà  ...  Tom is going to take care of it <end>\n",
              "174528                voi dovete essere ottimisti  ...        You have to be optimistic <end>\n",
              "81155                faremmo meglio ad andare tom  ...           We would better go Tom <end>\n",
              "12494                        li avevo dimenticati  ...                    I forgot them <end>\n",
              "219050              quando avete finito il lavoro  ...     When did you finish the work <end>\n",
              "201622              tom è qui per proteggere mary  ...      Tom is here to protect Mary <end>\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5L8ZhCi8Fs9"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSRfvhlb8LcD"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, validation = train_test_split(data, test_size=0.2)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7yXaG8k8Ml5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad141016-6407-4376-cb0e-28a66656f004"
      },
      "source": [
        "print(train.shape, validation.shape)\n",
        "# for one sentence we will be adding <end> token so that the tokanizer learns the word <end>\n",
        "# with this we can use only one tokenizer for both encoder output and decoder output\n",
        "train.iloc[0]['english_inp']= str(train.iloc[0]['english_inp'])+' <end>'\n",
        "train.iloc[0]['english_out']= str(train.iloc[0]['english_out'])+' <end>'"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(275852, 3) (68964, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAJoNGfr8ZPG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "8017b977-36b0-41f4-de94-0bd92faafda1"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>italian</th>\n",
              "      <th>english_inp</th>\n",
              "      <th>english_out</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>311020</th>\n",
              "      <td>tutte le mie figlie vogliono andare a boston</td>\n",
              "      <td>&lt;start&gt; All of my children want to go to Bosto...</td>\n",
              "      <td>All of my children want to go to Boston &lt;end&gt; ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106806</th>\n",
              "      <td>dica a tom che non sono a casa</td>\n",
              "      <td>&lt;start&gt; Tell Tom I am not home</td>\n",
              "      <td>Tell Tom I am not home &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124058</th>\n",
              "      <td>tom non è più vivo</td>\n",
              "      <td>&lt;start&gt; Tom is no longer alive</td>\n",
              "      <td>Tom is no longer alive &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138134</th>\n",
              "      <td>ti staranno guardando</td>\n",
              "      <td>&lt;start&gt; They will be watching you</td>\n",
              "      <td>They will be watching you &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7312</th>\n",
              "      <td>chiami qualcuno</td>\n",
              "      <td>&lt;start&gt; Call someone</td>\n",
              "      <td>Call someone &lt;end&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             italian  ...                                        english_out\n",
              "311020  tutte le mie figlie vogliono andare a boston  ...  All of my children want to go to Boston <end> ...\n",
              "106806                dica a tom che non sono a casa  ...                       Tell Tom I am not home <end>\n",
              "124058                            tom non è più vivo  ...                       Tom is no longer alive <end>\n",
              "138134                         ti staranno guardando  ...                    They will be watching you <end>\n",
              "7312                                 chiami qualcuno  ...                                 Call someone <end>\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "643DDFNi8Fde"
      },
      "source": [
        "tknizer_ita = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
        "tknizer_ita.fit_on_texts(train['italian'].values)\n",
        "tknizer_eng = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
        "tknizer_eng.fit_on_texts(train['english_inp'].values)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rTodkqnWSHS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "979621e5-1a42-4891-899b-416d122e1a3c"
      },
      "source": [
        "train['italian'].values"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['tutte le mie figlie vogliono andare a boston',\n",
              "       'dica a tom che non sono a casa', 'tom non è più vivo', ...,\n",
              "       'sono molto felice di incontrarla oggi', 'sembrate intelligenti',\n",
              "       'ci hanno trattati come persone di famiglia'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzgLpzJ-8gcY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58c8b039-36c1-4051-a0c4-d0539cecea66"
      },
      "source": [
        "vocab_size_eng=len(tknizer_eng.word_index.keys())\n",
        "print(vocab_size_eng)\n",
        "vocab_size_ita=len(tknizer_ita.word_index.keys())\n",
        "print(vocab_size_ita)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12816\n",
            "26224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqX4HLxq8iDn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df3e43d3-a857-4c7a-9d50-88afe9f758ee"
      },
      "source": [
        "tknizer_eng.word_index['<start>'], tknizer_eng.word_index['<end>']"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 10103)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkyWKGfo9GGF",
        "outputId": "4594707e-1f28-4b90-9a24-450d6bbad5c0"
      },
      "source": [
        "!wget https://www.dropbox.com/s/ddkmtqz01jc024u/glove.6B.100d.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-26 08:40:23--  https://www.dropbox.com/s/ddkmtqz01jc024u/glove.6B.100d.txt\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.80.18, 2620:100:601c:18::a27d:612\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.80.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/ddkmtqz01jc024u/glove.6B.100d.txt [following]\n",
            "--2021-05-26 08:40:24--  https://www.dropbox.com/s/raw/ddkmtqz01jc024u/glove.6B.100d.txt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucc42d30e4f55c7000b9573f4972.dl.dropboxusercontent.com/cd/0/inline/BPOEuc_DvZ_syGkCJmLjYCCgMQYfrtBdyNtbcfPTGHpvk3Tp8vqEhZdlFWXB5YAfZ8w-6bJZWNKGY0vnm2vu4UQkTS-Mhw0qXH_WECa0FKu6F3Owyiof00ZpnVrhzk_pDbZhzezcQzMwb3r1ktBf7JCs/file# [following]\n",
            "--2021-05-26 08:40:25--  https://ucc42d30e4f55c7000b9573f4972.dl.dropboxusercontent.com/cd/0/inline/BPOEuc_DvZ_syGkCJmLjYCCgMQYfrtBdyNtbcfPTGHpvk3Tp8vqEhZdlFWXB5YAfZ8w-6bJZWNKGY0vnm2vu4UQkTS-Mhw0qXH_WECa0FKu6F3Owyiof00ZpnVrhzk_pDbZhzezcQzMwb3r1ktBf7JCs/file\n",
            "Resolving ucc42d30e4f55c7000b9573f4972.dl.dropboxusercontent.com (ucc42d30e4f55c7000b9573f4972.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:601c:15::a27d:60f\n",
            "Connecting to ucc42d30e4f55c7000b9573f4972.dl.dropboxusercontent.com (ucc42d30e4f55c7000b9573f4972.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 347116733 (331M) [text/plain]\n",
            "Saving to: ‘glove.6B.100d.txt’\n",
            "\n",
            "glove.6B.100d.txt   100%[===================>] 331.04M   135MB/s    in 2.5s    \n",
            "\n",
            "2021-05-26 08:40:28 (135 MB/s) - ‘glove.6B.100d.txt’ saved [347116733/347116733]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6ZA3h929Bf0"
      },
      "source": [
        "embeddings_index = dict()\n",
        "f = open('glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size_eng+1, 100))\n",
        "for word, i in tknizer_eng.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8RDrP4xKabR"
      },
      "source": [
        "## <font color='blue'>**Implement custom encoder decoder**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtCni9R__bal"
      },
      "source": [
        "class Dataset:\n",
        "    def __init__(self, data, tknizer_ita, tknizer_eng, max_len):\n",
        "        self.encoder_inps = data['italian'].values\n",
        "        self.decoder_inps = data['english_inp'].values\n",
        "        self.decoder_outs = data['english_out'].values\n",
        "        self.tknizer_eng = tknizer_eng\n",
        "        self.tknizer_ita = tknizer_ita\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        self.encoder_seq = self.tknizer_ita.texts_to_sequences([self.encoder_inps[i]]) # need to pass list of values\n",
        "        self.decoder_inp_seq = self.tknizer_eng.texts_to_sequences([self.decoder_inps[i]])\n",
        "        self.decoder_out_seq = self.tknizer_eng.texts_to_sequences([self.decoder_outs[i]])\n",
        "\n",
        "        self.encoder_seq = pad_sequences(self.encoder_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
        "        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
        "        self.decoder_out_seq = pad_sequences(self.decoder_out_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
        "        return self.encoder_seq, self.decoder_inp_seq, self.decoder_out_seq\n",
        "\n",
        "    def __len__(self): # your model.fit_gen requires this function\n",
        "        return len(self.encoder_inps)\n",
        "\n",
        "    \n",
        "class Dataloder(tf.keras.utils.Sequence):    \n",
        "    def __init__(self, dataset, batch_size=1):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.indexes = np.arange(len(self.dataset.encoder_inps))\n",
        "\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        start = i * self.batch_size\n",
        "        stop = (i + 1) * self.batch_size\n",
        "        data = []\n",
        "        for j in range(start, stop):\n",
        "            data.append(self.dataset[j])\n",
        "\n",
        "        batch = [np.squeeze(np.stack(samples, axis=1), axis=0) for samples in zip(*data)]\n",
        "        # we are creating data like ([italian, english_inp], english_out) these are already converted into seq\n",
        "        return tuple([[batch[0],batch[1]],batch[2]])\n",
        "\n",
        "    def __len__(self):  # your model.fit_gen requires this function\n",
        "        return len(self.indexes) // self.batch_size\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = np.random.permutation(self.indexes)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWIoZF5IAXYB",
        "outputId": "3596cb55-40a5-4491-deb9-72bdc4c28a59"
      },
      "source": [
        "print(len(tknizer_ita.word_counts))\n",
        "print(len(tknizer_eng.word_counts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26300\n",
            "12887\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPlq-5GOAThA",
        "outputId": "c0e1c514-eb0d-4e75-bda5-2c5734f5689d"
      },
      "source": [
        "train_dataset = Dataset(train, tknizer_ita, tknizer_eng, 20)\n",
        "test_dataset  = Dataset(validation, tknizer_ita, tknizer_eng, 20)\n",
        "\n",
        "train_dataloader = Dataloder(train_dataset, batch_size=1024)\n",
        "test_dataloader = Dataloder(test_dataset, batch_size=1024)\n",
        "\n",
        "\n",
        "print(train_dataloader[0][0][0].shape, train_dataloader[0][0][1].shape, train_dataloader[0][1].shape)\n",
        "\n",
        "# train_dataloader[0][0][0] -> eng_input (not available in testing)\n",
        "# train_dataloader[0][0][0] -> eng_out (sentence to predict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1024, 20) (1024, 20) (1024, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A45uc0JILMlV"
      },
      "source": [
        "<font color='blue'>**Encoder**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cex2XfCLOew"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    '''\n",
        "    Encoder model -- That takes a input sequence and returns encoder-outputs,encoder_final_state_h,encoder_final_state_c\n",
        "    '''\n",
        "\n",
        "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
        "        #Initialize Embedding layer\n",
        "        #Intialize Encoder LSTM layer\n",
        "        super().__init__()\n",
        "        self.inp_vocab_size = inp_vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.input_length = input_length\n",
        "        self.lstm_units= lstm_size\n",
        "        self.lstm_output = 0\n",
        "        self.lstm_state_h=0\n",
        "        self.lstm_state_c=0\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        self.embedding = Embedding(input_dim=self.inp_vocab_size, output_dim=self.embedding_size, input_length=self.input_length,\n",
        "                           mask_zero=True, name=\"embedding_layer_encoder\")\n",
        "        self.lstm = LSTM(self.lstm_units, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
        "\n",
        "\n",
        "    def call(self,input_sequence,states=[]):\n",
        "        '''\n",
        "          This function takes a sequence input and the initial states of the encoder.\n",
        "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n",
        "          returns -- encoder_output, last time step's hidden and cell state\n",
        "        '''\n",
        "        input_embedding = self.embedding(input_sequence)\n",
        "        self.lstm_output, self.lstm_state_h, self.lstm_state_c = self.lstm(input_embedding)\n",
        "        return self.lstm_output, self.lstm_state_h, self.lstm_state_c\n",
        "\n",
        "    def initialize_states(self,batch_size):\n",
        "      '''\n",
        "      Given a batch size it will return intial hidden state and intial cell state.\n",
        "      If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n",
        "      '''\n",
        "      return np.zeros(shape=(batch_size,self.lstm_units))\n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UROHV3Co6Yi0",
        "outputId": "c9e15afb-8a6f-4946-b503-3dbb2f43cfd9"
      },
      "source": [
        "np.zeros(shape=(2000,64)).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtbOI3VwLOe0"
      },
      "source": [
        "<font color='orange'>**Grader function - 1**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziSqOgmhLOe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d146059-087f-42a4-c010-0c2ded5e4ee5"
      },
      "source": [
        "def grader_check_encoder():\n",
        "    '''\n",
        "        vocab-size: Unique words of the input language,\n",
        "        embedding_size: output embedding dimension for each word after embedding layer,\n",
        "        lstm_size: Number of lstm units,\n",
        "        input_length: Length of the input sentence,\n",
        "        batch_size\n",
        "    '''\n",
        "    vocab_size=10\n",
        "    embedding_size=20\n",
        "    lstm_size=32\n",
        "    input_length=10\n",
        "    batch_size=16\n",
        "    #Intialzing encoder \n",
        "    encoder=Encoder(vocab_size,embedding_size,lstm_size,input_length)\n",
        "    input_sequence=tf.random.uniform(shape=[batch_size,input_length],maxval=vocab_size,minval=0,dtype=tf.int32)\n",
        "    print(\"Input seq : \",input_sequence.shape)\n",
        "    #Intializing encoder initial states\n",
        "    initial_state=encoder.initialize_states(batch_size)\n",
        "    \n",
        "    encoder_output,state_h,state_c=encoder(input_sequence,initial_state)\n",
        "    \n",
        "    assert(encoder_output.shape==(batch_size,input_length,lstm_size) and state_h.shape==(batch_size,lstm_size) and state_c.shape==(batch_size,lstm_size))\n",
        "    return True\n",
        "print(grader_check_encoder())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input seq :  (16, 10)\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1ES1-sJLOe4"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    '''\n",
        "    Encoder model -- That takes a input sequence and returns output sequence\n",
        "    '''\n",
        "\n",
        "    def __init__(self,out_vocab_size,embedding_size,lstm_size,input_length):\n",
        "\n",
        "        #Initialize Embedding layer\n",
        "        #Intialize Decoder LSTM layer\n",
        "        super().__init__()\n",
        "        self.out_vocab_size = out_vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.dec_units = lstm_size\n",
        "        self.input_length = input_length\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.embedding = Embedding(input_dim=self.out_vocab_size, output_dim=self.embedding_size, input_length=self.input_length,\n",
        "                           mask_zero=True, name=\"embedding_layer_decoder\")\n",
        "        self.lstm = LSTM(self.dec_units, return_sequences=True, return_state=True, name=\"Decoder_LSTM\")\n",
        "\n",
        "\n",
        "    def call(self,input_sequence,initial_states):\n",
        "        '''\n",
        "          This function takes a sequence input and the initial states of the encoder.\n",
        "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to decoder_lstm\n",
        "        \n",
        "          returns -- decoder_output,decoder_final_state_h,decoder_final_state_c\n",
        "        '''\n",
        "        target_embed = self.embedding(input_sequence)\n",
        "        # print(\"Dec-Target Embedding shape: \",target_embed.shape)\n",
        "        # print(\"Dec-Initial stape: \",initial_states[0].shape,initial_states[1].shape)\n",
        "        # print(initial_states.shape)\n",
        "        # decoder_output, decoder_final_state_h,decoder_final_state_c = self.lstm(target_embedd, initial_state=[state_h, state_c])\n",
        "        decoder_output, decoder_final_state_h,decoder_final_state_c = self.lstm(target_embed, initial_state=initial_states)\n",
        "        return decoder_output, decoder_final_state_h, decoder_final_state_c \n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq-I0SUbLOe8"
      },
      "source": [
        "<font color='orange'>**Grader function - 2**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B0gokgKLOe8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23d13326-042a-4dc5-9f29-9653579fc3c5"
      },
      "source": [
        "def grader_decoder():\n",
        "    '''\n",
        "        out_vocab_size: Unique words of the target language,\n",
        "        embedding_size: output embedding dimension for each word after embedding layer,\n",
        "        dec_units: Number of lstm units in decoder,\n",
        "        input_length: Length of the input sentence,\n",
        "        batch_size\n",
        "    '''\n",
        "    out_vocab_size=13 \n",
        "    embedding_dim=12 \n",
        "    input_length=10\n",
        "    dec_units=16 \n",
        "    batch_size=32\n",
        "    \n",
        "    target_sentences=tf.random.uniform(shape=(batch_size,input_length),maxval=10,minval=0,dtype=tf.int32)\n",
        "    encoder_output=tf.random.uniform(shape=[batch_size,input_length,dec_units])\n",
        "    state_h=tf.random.uniform(shape=[batch_size,dec_units])\n",
        "    state_c=tf.random.uniform(shape=[batch_size,dec_units])\n",
        "    states=[state_h,state_c]\n",
        "    decoder=Decoder(out_vocab_size, embedding_dim, dec_units,input_length )\n",
        "    print(target_sentences.shape)\n",
        "    output,_,_=decoder(target_sentences, states)\n",
        "    assert(output.shape==(batch_size,input_length,dec_units))\n",
        "    return True\n",
        "print(grader_decoder())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 10)\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXrIj4scLOe_"
      },
      "source": [
        "class Encoder_decoder(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,encoder_inputs_length,decoder_inputs_length, ita_vocab_size, eng_vocab_size):\n",
        "        \n",
        "        super().__init__() \n",
        "        self.encoder_inputs_length =encoder_inputs_length\n",
        "        self.decoder_inputs_length = decoder_inputs_length\n",
        "        self.output_vocab_size = eng_vocab_size\n",
        "        self.input_vocab_size  = ita_vocab_size\n",
        "\n",
        "        \n",
        "    \n",
        "    def build(self, input_shape):\n",
        "      # self,inp_vocab_size,embedding_size,lstm_size,input_length)\n",
        "      self.encoder = Encoder(inp_vocab_size=self.input_vocab_size, embedding_size=20, input_length=self.encoder_inputs_length, lstm_size=64)\n",
        "      # (self,out_vocab_size,embedding_size,lstm_size,input_length)\n",
        "      self.decoder = Decoder(out_vocab_size=self.output_vocab_size, embedding_size=20, input_length=self.decoder_inputs_length, lstm_size=64)\n",
        "      self.dense   = Dense(self.output_vocab_size, activation='softmax',name=\"Enc_Dec_Dense\")\n",
        "\n",
        "\n",
        "    def call(self,data):\n",
        "        '''\n",
        "        A. Pass the input sequence to Encoder layer -- Return encoder_output,encoder_final_state_h,encoder_final_state_c\n",
        "        B. Pass the target sequence to Decoder layer with intial states as encoder_final_state_h,encoder_final_state_C\n",
        "        C. Pass the decoder_outputs into Dense layer\n",
        "        Return decoder_outputs\n",
        "        '''\n",
        "        input,output = data[0], data[1]\n",
        "        encoder_output, encoder_h, encoder_c = self.encoder(input, self.encoder.initialize_states(batch_size=1024)) # you need to pass states too\n",
        "        decoder_output, decoder_h, decoder_h= self.decoder(output, [encoder_h, encoder_c])\n",
        "        output= self.dense(decoder_output)\n",
        "        return output\n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl_deiKcYrPO"
      },
      "source": [
        "train_dataset = Dataset(train, tknizer_ita, tknizer_eng, 20)\n",
        "test_dataset  = Dataset(validation, tknizer_ita, tknizer_eng, 20)\n",
        "\n",
        "train_dataloader = Dataloder(train_dataset, batch_size=1024)\n",
        "test_dataloader = Dataloder(test_dataset, batch_size=1024)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rk7kgDdk4rHU",
        "outputId": "115b62cb-43d9-4c8e-e21d-669e3bd2f91b"
      },
      "source": [
        "print(vocab_size_ita)\n",
        "print(vocab_size_eng)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26300\n",
            "12887\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYK33GesAeYc",
        "outputId": "81cc040e-06ee-4e07-f23a-d3d520d144bb"
      },
      "source": [
        "# encoder_inputs_length,decoder_inputs_length, output_vocab_size, eng_vocab_size\n",
        "model  = Encoder_decoder(encoder_inputs_length=20,decoder_inputs_length=20,ita_vocab_size=vocab_size_ita+1, eng_vocab_size = vocab_size_eng+1)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "model.compile(optimizer=optimizer,loss='sparse_categorical_crossentropy')\n",
        "\n",
        "model.fit_generator(train_dataloader, epochs=2, validation_data=test_dataloader)\n",
        "# model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3704: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable.debug_mode()`.\n",
            "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "269/269 [==============================] - 72s 267ms/step - loss: 2.0735 - val_loss: 1.8107\n",
            "Epoch 2/2\n",
            "269/269 [==============================] - 72s 267ms/step - loss: 1.7630 - val_loss: 1.6816\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f98280afc90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcL61dJXLOfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df35daeb-87f1-41a1-e2c8-be44303cdf61"
      },
      "source": [
        "# Create an object of encoder_decoder Model class, \n",
        "# Compile the model and fit the model\n",
        "model.layers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<__main__.Encoder at 0x7f98281dd990>,\n",
              " <__main__.Decoder at 0x7f98281dd190>,\n",
              " <tensorflow.python.keras.layers.core.Dense at 0x7f982818a190>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkARSlZgLOfE"
      },
      "source": [
        "def predict(input_sentence):\n",
        "  '''\n",
        "  A. Given input sentence, convert the sentence into integers using tokenizer used earlier\n",
        "  B. Pass the input_sequence to encoder. we get encoder_outputs, last time step hidden and cell state\n",
        "  C. Initialize index of <start> as input to decoder. and encoder final states as input_states to decoder\n",
        "  D. till we reach max_length of decoder or till the model predicted word <end>:\n",
        "         predicted_out,state_h,state_c=model.layers[1](dec_input,states)\n",
        "         pass the predicted_out to the dense layer\n",
        "         update the states=[state_h,state_c]\n",
        "         And get the index of the word with maximum probability of the dense layer output, using the tokenizer(word index) get the word and then store it in a string.\n",
        "         Update the input_to_decoder with current predictions\n",
        "  F. Return the predicted sentence\n",
        "  '''\n",
        "  input_length = 20\n",
        "  lstm_units = 64\n",
        "  encoder_seq = tknizer_ita.texts_to_sequences([input_sentence]) # need to pass list of values\n",
        "  encoder_seq = pad_sequences(encoder_seq, maxlen=input_length, dtype='int32', padding='post')\n",
        "  # print(\"TOKENIZED DATA :\",encoder_seq.shape)\n",
        "\n",
        "  encoder_output, enc_h, enc_c  = model.layers[0](encoder_seq, np.zeros(shape=(1,64)))\n",
        "  # print(\"ENCODER OUTPUT SHAPES: \",encoder_output.shape, enc_h.shape, enc_c.shape)\n",
        "\n",
        "  dec_input = np.array([[1]+[0]*19]) # max_len=20 and index of <start> is 1\n",
        "  # print(dec_input.shape)\n",
        "  states = [enc_h, enc_c]\n",
        "  result_sentence = \"\"\n",
        "\n",
        "  # print(\"DECODER PART STARTS....\")\n",
        "  for i in range(20):\n",
        "    # print(dec_input[0])\n",
        "    predicted_out,state_h,state_c=model.layers[1](dec_input,states)\n",
        "    dense_op = model.layers[2](predicted_out)\n",
        "    states = [state_h,state_c] #update states to be used in next iter\n",
        "\n",
        "    pred_index = np.argmax(dense_op.numpy()[0][0])\n",
        "    pred_word = tknizer_eng.index_word[pred_index] # get predicted word \n",
        "    if pred_word == \"<end>\":\n",
        "      break\n",
        "    result_sentence += pred_word+\" \"\n",
        "    # print(\"Output for TS %d = %s\"%(i, str(pred_word)))\n",
        "\n",
        "    # give index of pred_word as input to next timestep\n",
        "    dec_input[0][i] = pred_index\n",
        "\n",
        "  return result_sentence\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhL0d4erN1pY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "f8ec940f-48d8-4103-bf7d-6171eb7d6f8a"
      },
      "source": [
        "res = predict(\"tom la sta leggendo\")\n",
        "res"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'i you '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKAVqZ2fztki"
      },
      "source": [
        "import nltk.translate.bleu_score as bleu\n",
        "import random\n",
        "\n",
        "\n",
        "def aveg_bleu_scores(test_data,n):\n",
        "  sample_list = random.sample(range(len(test_data)),n)\n",
        "  average_bleu = 0\n",
        "  pred_data = []\n",
        "\n",
        "  for i in sample_list:\n",
        "    test_sentence, true_sentence = test_data.iloc[i,[0,1]]\n",
        "    pred_sentence = predict(test_sentence)\n",
        "    bleu_score = bleu.sentence_bleu([true_sentence.split()],pred_sentence.split())\n",
        "    average_bleu += bleu_score\n",
        "\n",
        "    pred_data.append((true_sentence, pred_sentence))\n",
        "  \n",
        "  return (average_bleu/n, pred_data)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLDfx7Fg06Cr",
        "outputId": "233b3fac-03fe-4f2a-f615-1a206198a0d7"
      },
      "source": [
        "# Predict on 1000 random sentences on test data and calculate the average BLEU score of these sentences.\n",
        "# https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "test_df = validation.copy()\n",
        "test_df = test_df[[\"italian\",\"english_out\"]]\n",
        "test_df[\"english_out\"] = test_df[\"english_out\"].apply(lambda x: x.split(\"<end>\")[0])\n",
        "\n",
        "\n",
        "res = aveg_bleu_scores(test_df,10)\n",
        "print(res[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.07598356856515925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxWFDxZXLOfJ"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZhX3K9GLOfJ"
      },
      "source": [
        "## Task -2: Including Attention mechanisum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3d7GeBMGbsJ"
      },
      "source": [
        "1. Use the preprocessed data from Task-1\n",
        "\n",
        "2. You have to implement an Encoder and Decoder architecture with  \n",
        "attention as discussed in the reference notebook.\n",
        "\n",
        "    * Encoder   - with 1 layer LSTM <br>\n",
        "    * Decoder   - with 1 layer LSTM<br>\n",
        "    * attention -  (Please refer the <a href= 'https://drive.google.com/file/d/1z_bnc-3aubKawbR6q8wyI6Mh5ho2R1aZ/view?usp=sharing'>**reference notebook**</a> to know more about the attention mechanism.)\n",
        "3. In Global attention, we have 3 types of scoring functions(as discussed in the reference notebook).\n",
        " As a part of this assignment **you need to create 3 models for each scoring function**\n",
        "<img src='https://i.imgur.com/iD2jZo3.png'>\n",
        "\n",
        "    * In model 1 you need to implemnt \"dot\" score function\n",
        "    * In model 2 you need to implemnt \"general\" score function\n",
        "    * In model 3 you need to implemnt \"concat\" score function.<br>\n",
        "    \n",
        " **Please do add the markdown titles for each model so that we can have a better look at the code and verify.**\n",
        "4. It is mandatory to train the model with simple model.fit() only, Donot train the model with custom GradientTape()\n",
        "\n",
        "5. Using attention weights, you can plot the attention plots, \n",
        "please plot those for 2-3 examples. You can check about those in <a href=\"https://www.tensorflow.org/tutorials/text/nmt_with_attention#translate\">this</a>\n",
        "\n",
        "6. The attention layer has to be written by yourself only. \n",
        "The main objective of this assignment is to read and implement a paper on yourself so please do it yourself.  \n",
        "\n",
        "7. Please implement the class **onestepdecoder** as mentioned in the assignment instructions.\n",
        "\n",
        "8. You can use any tf.Keras highlevel API's to build and train the models. \n",
        " Check the reference notebook for better understanding.\n",
        "\n",
        "9. Use BLEU score as metric to evaluate your model. You can use any loss function you need.\n",
        "\n",
        "10. You have to use Tensorboard to plot the Graph, Scores and histograms of gradients. \n",
        "\n",
        "11. Resources:\n",
        "    a. Check the reference notebook\n",
        "    b. <a href=\"https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\">Resource 1</a>\n",
        "    c. <a href=\"https://www.tensorflow.org/tutorials/text/nmt_with_attention\">Resource 2</a>\n",
        "    d. <a href=\"https://stackoverflow.com/questions/44238154/what-is-the-difference-between-luong-attention-and-bahdanau-attention#:~:text=Luong%20attention%20used%20top%20hidden,hidden%20state%20at%20time%20t.\">Resource 3</a>\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU4KIsGxLOfK"
      },
      "source": [
        "### <font color='blue'>**Implement custom encoder decoder and attention layers**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMm3ADQDLOfK"
      },
      "source": [
        "<font color='blue'>**Encoder**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx_5NA24KzRp"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    '''\n",
        "    Encoder model -- That takes a input sequence and returns output sequence\n",
        "    '''\n",
        "\n",
        "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
        "      super().__init__()\n",
        "      self.inp_vocab_size = inp_vocab_size\n",
        "      self.embedding_size = embedding_size\n",
        "      self.lstm_size = lstm_size\n",
        "      self.input_length = input_length\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "      self.embedding = Embedding(input_dim=self.inp_vocab_size, output_dim=self.embedding_size,\n",
        "                                 input_length = self.input_length, mask_zero=True,name=\"Encoder_Embedding_Layer\")\n",
        "      self.lstm = LSTM(units=self.lstm_size, return_state = True, return_sequences=True, name=\"Encoder_LSTM_Layer\")\n",
        "\n",
        "    def call(self,input_sequence,states):\n",
        "      '''\n",
        "          This function takes a sequence input and the initial states of the encoder.\n",
        "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n",
        "          returns -- All encoder_outputs, last time steps hidden and cell state\n",
        "      '''\n",
        "      embed = self.embedding(input_sequence)\n",
        "      encoder_output, encoder_h, encoder_c = self.lstm(embed)\n",
        "      return encoder_output, encoder_h, encoder_c\n",
        "\n",
        "    \n",
        "    def initialize_states(self,batch_size):\n",
        "      '''\n",
        "      Given a batch size it will return intial hidden state and intial cell state.\n",
        "      If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n",
        "      '''\n",
        "      initial_h = np.zeros(shape=(batch_size, self.lstm_size))\n",
        "      initial_c = np.zeros(shape=(batch_size, self.lstm_size))\n",
        "\n",
        "      return (initial_h, initial_c)\n",
        "      \n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ub9aN-hK244"
      },
      "source": [
        "<font color='cyan'>**Grader function - 1**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRoe65b9LB0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4bd5e45-2f81-4b58-8d40-67260afdd428"
      },
      "source": [
        "def grader_check_encoder():\n",
        "    \n",
        "    '''\n",
        "        vocab-size: Unique words of the input language,\n",
        "        embedding_size: output embedding dimension for each word after embedding layer,\n",
        "        lstm_size: Number of lstm units in encoder,\n",
        "        input_length: Length of the input sentence,\n",
        "        batch_size\n",
        "    '''\n",
        "    \n",
        "    vocab_size=10\n",
        "    embedding_size=20\n",
        "    lstm_size=32\n",
        "    input_length=10\n",
        "    batch_size=16\n",
        "    encoder=Encoder(vocab_size,embedding_size,lstm_size,input_length)\n",
        "    input_sequence=tf.random.uniform(shape=[batch_size,input_length],maxval=vocab_size,minval=0,dtype=tf.int32)\n",
        "    initial_state=encoder.initialize_states(batch_size)\n",
        "    encoder_output,state_h,state_c=encoder(input_sequence,initial_state)\n",
        "    \n",
        "    assert(encoder_output.shape==(batch_size,input_length,lstm_size) and state_h.shape==(batch_size,lstm_size) and state_c.shape==(batch_size,lstm_size))\n",
        "    return True\n",
        "print(grader_check_encoder())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXn278lhLYRM"
      },
      "source": [
        "<font color='blue'>**Attention**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikzOvqRvYRRH"
      },
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "  '''\n",
        "    Class the calculates score based on the scoring_function using Bahdanu attention mechanism.\n",
        "  '''\n",
        "  def __init__(self,scoring_function, att_units):\n",
        "    super().__init__()\n",
        "    self.scoring_function = scoring_function\n",
        "    self.att_units = att_units\n",
        "\n",
        "\n",
        "    # Please go through the reference notebook and research paper to complete the scoring functions\n",
        "    if self.scoring_function=='dot':\n",
        "      # Intialize variables needed for Dot score function here\n",
        "      pass\n",
        "    if scoring_function == 'general':\n",
        "      # Intialize variables needed for General score function here\n",
        "      pass\n",
        "    elif scoring_function == 'concat':\n",
        "      # Intialize variables needed for Concat score function here\n",
        "      pass\n",
        "  \n",
        "  \n",
        "  def call(self,decoder_hidden_state,encoder_output):\n",
        "    '''\n",
        "      Attention mechanism takes two inputs current step -- decoder_hidden_state and all the encoder_outputs.\n",
        "      * Based on the scoring function we will find the score or similarity between decoder_hidden_state and encoder_output.\n",
        "        Multiply the score function with your encoder_outputs to get the context vector.\n",
        "        Function returns context vector and attention weights(softmax - scores)\n",
        "    '''\n",
        "    \n",
        "    if self.scoring_function == 'dot':\n",
        "        # print(\"1.Hidden State : \",decoder_hidden_state.shape)\n",
        "        # print(\"2.Enc OUtput :\", encoder_output.shape)\n",
        "        decoder_hidden_state = Reshape(target_shape=(decoder_hidden_state.shape[1],1))(decoder_hidden_state)\n",
        "        # print(\"2.Hidden State resize: \",decoder_hidden_state.shape)\n",
        "\n",
        "        dot_layer = Dot(axes=(2,1))([encoder_output,decoder_hidden_state])\n",
        "        # print(\"3.Dot Product : \",dot_layer.shape)\n",
        "        dot_layer = tf.squeeze(dot_layer)\n",
        "        # print(\"3.Dot Product Reshape: \",dot_layer.shape)\n",
        "\n",
        "        softmax_layer = Softmax()(dot_layer)\n",
        "        # print(\"4.Softmax Layer :\",softmax_layer.shape)\n",
        "        # print(\"4->Sum after softmax: \",sum(x for x in softmax_layer[0].numpy()))\n",
        "\n",
        "        # weighted context_vector\n",
        "        weights_squeezed = softmax_layer # bat,timesteps\n",
        "        encoder_ops = encoder_output # bat,timesteps,enc_units\n",
        "\n",
        "        # multiply weights and add to get context_vector\n",
        "        # weights = Reshape(target_shape=(weights_squeezed.shape[1],1))(weights_squeezed) # bat,timesteps, 1\n",
        "        weights = tf.expand_dims(weights_squeezed, axis=-1)\n",
        "        multiply_weights = tf.multiply(encoder_ops, weights)\n",
        "        # print(\"5. After multiplying weights: \",multiply_weights.shape)\n",
        "\n",
        "        context_vector = tf.math.reduce_sum(multiply_weights, axis=-2)\n",
        "        # print(\"5. Context Vector: \",context_vector.shape)\n",
        "\n",
        "        return context_vector, weights\n",
        "        \n",
        "    elif self.scoring_function == 'general':\n",
        "      decoder_hidden_state = Dense(encoder_output.shape[-1])(decoder_hidden_state)\n",
        "      decoder_hidden_state = Reshape(target_shape=(decoder_hidden_state.shape[1],1))(decoder_hidden_state)\n",
        "\n",
        "      dot_layer = Dot(axes=(2,1))([encoder_output,decoder_hidden_state])\n",
        "      squeezed_dot_layer = tf.squeeze(dot_layer)\n",
        "      softmax_layer_weights = Softmax()(squeezed_dot_layer) # bat, timesteps\n",
        "\n",
        "      # encoder_ops = encoder_output.numpy() # bat,timesteps,enc_units\n",
        "\n",
        "      # multiply weights and add to get context_vector\n",
        "      weights = Reshape(target_shape=(softmax_layer_weights.shape[1],1))(softmax_layer_weights) # bat,timesteps, 1\n",
        "      multiply_weights = tf.multiply(encoder_output, weights)\n",
        "      context_vector = tf.math.reduce_sum(multiply_weights, axis=-2)\n",
        "\n",
        "      return context_vector, weights\n",
        "\n",
        "    elif self.scoring_function == 'concat':\n",
        "      k = 64\n",
        "      encoder_timesteps = encoder_output.shape[1]\n",
        "\n",
        "      dense_d = Dense(k)(decoder_hidden_state) #bat, K\n",
        "      dense_e = Dense(k)(encoder_output) # bat,time_steps, K\n",
        "\n",
        "      # concat and tanH\n",
        "      expand = tf.expand_dims(dense_d,axis=1)\n",
        "      tiled = tf.tile(expand, multiples=[1,encoder_timesteps,1])\n",
        "      concat = tf.concat([dense_e, tiled], axis=-1) #bat,time_steps, 2*K\n",
        "\n",
        "      tanh = tf.keras.activations.tanh(concat) #bat,time_steps, 2*K\n",
        "      w = Dense(1)(tanh) #bat,time_steps, 1\n",
        "\n",
        "      # multiply weights and add to get context_vector\n",
        "      weights = Reshape(target_shape=(w.shape[1],1))(w) # bat,timesteps, 1\n",
        "      multiply_weights = tf.multiply(encoder_output, weights)\n",
        "      context_vector = tf.math.reduce_sum(multiply_weights, axis=-2)\n",
        "\n",
        "      return context_vector, weights\n",
        "      \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExQDlxI9LuqK"
      },
      "source": [
        "<font color='cyan'>**Grader function - 2**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51x50h_TLrl9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9ddbde9-33f8-4672-8f09-e50c9e63fb2e"
      },
      "source": [
        "def grader_check_attention(scoring_fun):\n",
        "    \n",
        "    ''' \n",
        "        att_units: Used in matrix multiplications for scoring functions,\n",
        "        input_length: Length of the input sentence,\n",
        "        batch_size\n",
        "    '''\n",
        "    \n",
        "    input_length=10\n",
        "    batch_size=16\n",
        "    att_units=32\n",
        "    \n",
        "    state_h=tf.random.uniform(shape=[batch_size,att_units])\n",
        "    encoder_output=tf.random.uniform(shape=[batch_size,input_length,att_units])\n",
        "    attention=Attention(scoring_fun,att_units)\n",
        "    context_vector,attention_weights=attention(state_h,encoder_output)\n",
        "    assert(context_vector.shape==(batch_size,att_units) and attention_weights.shape==(batch_size,input_length,1))\n",
        "    return True\n",
        "\n",
        "print(grader_check_attention('dot'))\n",
        "print(grader_check_attention('general'))\n",
        "print(grader_check_attention('concat'))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic-FNEbfL2DN"
      },
      "source": [
        "<font color='blue'>**OneStepDecoder**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc8m7lmOL097"
      },
      "source": [
        "class One_Step_Decoder(tf.keras.Model):\n",
        "  def __init__(self,tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
        "    super().__init__()\n",
        "    self.tar_vocab_size = tar_vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.input_length = input_length \n",
        "    self.dec_units = dec_units\n",
        "    self.score_fun = score_fun\n",
        "    self.att_units = att_units\n",
        "\n",
        "    # Initialize decoder embedding layer, LSTM and any other objects needed\n",
        "  def build(self,input_shape):\n",
        "    self.embedding = Embedding(input_dim=self.tar_vocab_size, output_dim=self.embedding_dim,\n",
        "                              input_length=self.input_length, mask_zero=True,name=\"OHE_Embedding\")\n",
        "    self.lstm = LSTM(units=self.dec_units,return_state = True, return_sequences=True, name=\"OHE_LSTM_Layer\")\n",
        "    self.attention = Attention(self.score_fun,self.att_units)\n",
        "\n",
        "\n",
        "  def call(self,input_to_decoder, encoder_output, state_h,state_c):\n",
        "    '''\n",
        "        One step decoder mechanisim step by step:\n",
        "      A. Pass the input_to_decoder to the embedding layer and then get the output(batch_size,1,embedding_dim)\n",
        "      B. Using the encoder_output and decoder hidden state, compute the context vector.\n",
        "      C. Concat the context vector with the step A output\n",
        "      D. Pass the Step-C output to LSTM/GRU and get the decoder output and states(hidden and cell state)\n",
        "      E. Pass the decoder output to dense layer(vocab size) and store the result into output.\n",
        "      F. Return the states from step D, output from Step E, attention weights from Step -B\n",
        "    '''\n",
        "\n",
        "    embed = self.embedding(input_to_decoder) #32,1,12\n",
        "    context_vector, weights = self.attention(state_h, encoder_output) #32,16\n",
        "    context_vector_exp = tf.expand_dims(context_vector, axis=1) #32,16 -> 32,1,16\n",
        "    concat = tf.concat([embed,context_vector_exp],axis=-1) #32,1,28 \n",
        "\n",
        "    decoder_output, decoder_h, decoder_c = self.lstm(concat) #(32, 1, 16) (32, 16)\n",
        "    dense = Dense(self.tar_vocab_size)(decoder_output)  #(32, 1, 13)\n",
        "\n",
        "    output = tf.squeeze(dense)  #(32, 1, 13) -> (32,13)\n",
        "    # return output, state_h, state_c, weights, context_vector\n",
        "    return output, decoder_h, decoder_c, weights, context_vector\n",
        "    \n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_I8I4EIMAXq"
      },
      "source": [
        "<font color='cyan'>**Grader function - 3**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLEXhChnMC1k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9603388a-1c6c-42ca-8cb3-e934fbf4f0f1"
      },
      "source": [
        "def grader_onestepdecoder(score_fun):  \n",
        "    '''\n",
        "        tar_vocab_size: Unique words of the target language,\n",
        "        embedding_dim: output embedding dimension for each word after embedding layer,\n",
        "        dec_units: Number of lstm units in decoder,\n",
        "        att_units: Used in matrix multiplications for scoring functions in attention class,\n",
        "        input_length: Length of the target sentence,\n",
        "        batch_size   \n",
        "    '''\n",
        "    tar_vocab_size=13 \n",
        "    embedding_dim=12 \n",
        "    input_length=10\n",
        "    dec_units=16 \n",
        "    att_units=16\n",
        "    batch_size=32\n",
        "    onestepdecoder=One_Step_Decoder(tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units)\n",
        "    input_to_decoder=tf.random.uniform(shape=(batch_size,1),maxval=10,minval=0,dtype=tf.int32)\n",
        "    encoder_output=tf.random.uniform(shape=[batch_size,input_length,dec_units])\n",
        "    state_h=tf.random.uniform(shape=[batch_size,dec_units])\n",
        "    state_c=tf.random.uniform(shape=[batch_size,dec_units])\n",
        "    output,state_h,state_c,attention_weights,context_vector=onestepdecoder(input_to_decoder,encoder_output,state_h,state_c)\n",
        "    assert(output.shape==(batch_size,tar_vocab_size))\n",
        "    assert(state_h.shape==(batch_size,dec_units))\n",
        "    assert(state_c.shape==(batch_size,dec_units))\n",
        "    assert(attention_weights.shape==(batch_size,input_length,1))\n",
        "    assert(context_vector.shape==(batch_size,dec_units))\n",
        "    return True\n",
        "    \n",
        "print(grader_onestepdecoder('dot'))\n",
        "print(grader_onestepdecoder('general'))\n",
        "print(grader_onestepdecoder('concat'))\n",
        "    "
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FHrurjUMGAi"
      },
      "source": [
        "<font color='blue'>**Decoder**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV-x31rj6Hc4"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self,out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
        "      #Intialize necessary variables and create an object from the class onestepdecoder\n",
        "      super().__init__()\n",
        "      self.out_vocab_size = out_vocab_size\n",
        "      self.embedding_dim = embedding_dim\n",
        "      self.input_length = input_length \n",
        "      self.dec_units = dec_units\n",
        "      self.score_fun = score_fun\n",
        "      self.att_units = att_units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "      self.onestepdecoder = One_Step_Decoder(self.out_vocab_size, self.embedding_dim, \n",
        "                                             self.input_length, self.dec_units, self.score_fun, self.att_units)\n",
        "\n",
        "        \n",
        "    def call(self, input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state ):\n",
        "\n",
        "      #Initialize an empty Tensor array, that will store the outputs at each and every time step\n",
        "      tf_output = tf.TensorArray(dtype=tf.float32, size=self.input_length, name=\"tf_output_array\")\n",
        "      \n",
        "      \n",
        "      #Iterate till the length of the decoder input\n",
        "      for timestep in range(self.input_length):\n",
        "        # Call onestepdecoder for each token in decoder_input\n",
        "        output,state_h,state_c,attention_weights,context_vector = self.onestepdecoder(input_to_decoder[:,timestep:timestep+1],\n",
        "                                                                                      encoder_output,decoder_hidden_state,decoder_cell_state)\n",
        "        tf_output = tf_output.write(timestep, output)\n",
        "        decoder_hidden_state = state_h\n",
        "        decoder_cell_state = state_c\n",
        "      \n",
        "      # Return the tensor array\n",
        "      tf_output = tf.transpose(tf_output.stack(),[1,0,2])\n",
        "      return tf_output\n",
        "        \n",
        "        "
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxrL-P8bMJH6"
      },
      "source": [
        "<font color='cyan'>**Grader function - 4**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtbx6onFMJXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7ef8319-4752-49a3-cc29-666bc5a1b88d"
      },
      "source": [
        "def grader_decoder(score_fun):\n",
        "    \n",
        "    '''\n",
        "        out_vocab_size: Unique words of the target language,\n",
        "        embedding_dim: output embedding dimension for each word after embedding layer,\n",
        "        dec_units: Number of lstm units in decoder,\n",
        "        att_units: Used in matrix multiplications for scoring functions in attention class,\n",
        "        input_length: Length of the target sentence,\n",
        "        batch_size\n",
        "        \n",
        "    \n",
        "    '''\n",
        "    \n",
        "    out_vocab_size=13 \n",
        "    embedding_dim=12 \n",
        "    input_length=11\n",
        "    dec_units=16 \n",
        "    att_units=16\n",
        "    batch_size=32\n",
        "    \n",
        "    target_sentences=tf.random.uniform(shape=(batch_size,input_length),maxval=10,minval=0,dtype=tf.int32)\n",
        "    encoder_output=tf.random.uniform(shape=[batch_size,input_length,dec_units])\n",
        "    state_h=tf.random.uniform(shape=[batch_size,dec_units])\n",
        "    state_c=tf.random.uniform(shape=[batch_size,dec_units])\n",
        "    \n",
        "    decoder=Decoder(out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units)\n",
        "    output=decoder(target_sentences,encoder_output, state_h, state_c)\n",
        "    assert(output.shape==(batch_size,input_length,out_vocab_size))\n",
        "    return True\n",
        "print(grader_decoder('dot'))\n",
        "print(grader_decoder('general'))\n",
        "print(grader_decoder('concat'))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC1T1EOoMTqC"
      },
      "source": [
        "<font color='blue'>**Encoder Decoder model**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfqBIe20MT3D"
      },
      "source": [
        "class encoder_decoder(tf.keras.Model):\n",
        "  def __init__(self,inp_vocab_size,out_vocab_size, embedding_size,lstm_size, input_length, scoring_fun, att_units,batch_size):\n",
        "    #Intialize objects from encoder decoder\n",
        "    super().__init__()\n",
        "    self.inp_vocab_size = inp_vocab_size\n",
        "    self.out_vocab_size = out_vocab_size\n",
        "    self.embedding_size = embedding_size\n",
        "    self.lstm_size = lstm_size\n",
        "    self.input_length = input_length\n",
        "    self.scoring_fun = scoring_fun\n",
        "    self.att_units = att_units\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.encoder = Encoder(self.inp_vocab_size,self.embedding_size,self.lstm_size,self.input_length)\n",
        "    self.decoder=Decoder(self.out_vocab_size, self.embedding_size, self.input_length, self.lstm_size ,self.scoring_fun ,self.att_units)\n",
        "\n",
        "\n",
        "  def call(self,data):\n",
        "    #Intialize encoder states, Pass the encoder_sequence to the embedding layer\n",
        "    # Decoder initial states are encoder final states, Initialize it accordingly\n",
        "    # Pass the decoder sequence,encoder_output,decoder states to Decoder\n",
        "    # return the decoder output\n",
        "    input_sequences, target_sentences = data[0], data[1]\n",
        "    enc_initial_state = self.encoder.initialize_states(self.batch_size)\n",
        "    encoder_output, state_h, state_c = self.encoder(input_sequences,enc_initial_state)\n",
        "\n",
        "    # teacher training\n",
        "    decoder_output = self.decoder(target_sentences,encoder_output, state_h, state_c)\n",
        "    return decoder_output\n",
        "\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVRxB-FDMJWL"
      },
      "source": [
        "<font color='blue'>**Custom loss function**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QY_3izrXMs8y"
      },
      "source": [
        "\n",
        "# Refer https://www.tensorflow.org/tutorials/text/nmt_with_attention#define_the_optimizer_and_the_loss_function\n",
        "\n",
        "def custom_lossfunction(targets,logits):\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=False, reduction='none')\n",
        "  mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "  loss_ = loss_object(targets, logits)\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QlbWAqNNlqe"
      },
      "source": [
        "# Training Model with Dot "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAhFPrK-V0Ox",
        "outputId": "027cbe7b-395b-44fe-d014-2ff78facb710"
      },
      "source": [
        "train_dataset = Dataset(train, tknizer_ita, tknizer_eng, 20)\n",
        "test_dataset  = Dataset(validation, tknizer_ita, tknizer_eng, 20)\n",
        "train_dataloader = Dataloder(train_dataset, batch_size=1024)\n",
        "test_dataloader = Dataloder(test_dataset, batch_size=1024)\n",
        "print(train_dataloader[0][0][0].shape, train_dataloader[0][0][1].shape, train_dataloader[0][1].shape)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1024, 20) (1024, 20) (1024, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbfINkUiVcGO"
      },
      "source": [
        "model = encoder_decoder(inp_vocab_size=vocab_size_ita+1,\n",
        "                        out_vocab_size=vocab_size_eng+1, \n",
        "                        embedding_size=20,\n",
        "                        lstm_size=32,\n",
        "                        input_length=20, \n",
        "                        scoring_fun='dot',\n",
        "                        att_units=32,\n",
        "                        batch_size=1024)\n",
        "model.compile(optimizer=optimizer, loss=custom_lossfunction)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDceWNnBWPRH",
        "outputId": "0f33d283-dcf5-418b-ed9f-01916f8836b0"
      },
      "source": [
        "tf.config.run_functions_eagerly(True)\n",
        "model.fit_generator(train_dataloader, epochs=20, validation_data=test_dataloader)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3704: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable.debug_mode()`.\n",
            "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "269/269 [==============================] - 206s 767ms/step - loss: 4.6634 - val_loss: 4.7286\n",
            "Epoch 2/20\n",
            "269/269 [==============================] - 206s 766ms/step - loss: 4.7213 - val_loss: 4.7059\n",
            "Epoch 3/20\n",
            "269/269 [==============================] - 205s 763ms/step - loss: 4.6381 - val_loss: 3.4966\n",
            "Epoch 4/20\n",
            "269/269 [==============================] - 204s 760ms/step - loss: 3.3783 - val_loss: 3.2561\n",
            "Epoch 5/20\n",
            "269/269 [==============================] - 204s 758ms/step - loss: 3.2684 - val_loss: 3.2495\n",
            "Epoch 6/20\n",
            "269/269 [==============================] - 204s 759ms/step - loss: 3.2578 - val_loss: 3.2477\n",
            "Epoch 7/20\n",
            "269/269 [==============================] - 204s 760ms/step - loss: 3.2542 - val_loss: 3.2468\n",
            "Epoch 8/20\n",
            "269/269 [==============================] - 204s 758ms/step - loss: 3.2521 - val_loss: 3.2463\n",
            "Epoch 9/20\n",
            "269/269 [==============================] - 205s 762ms/step - loss: 3.2507 - val_loss: 3.2458\n",
            "Epoch 10/20\n",
            "269/269 [==============================] - 205s 761ms/step - loss: 3.2495 - val_loss: 3.2455\n",
            "Epoch 11/20\n",
            "269/269 [==============================] - 205s 760ms/step - loss: 3.2487 - val_loss: 3.2453\n",
            "Epoch 12/20\n",
            "269/269 [==============================] - 206s 767ms/step - loss: 3.2481 - val_loss: 3.2451\n",
            "Epoch 13/20\n",
            "269/269 [==============================] - 204s 757ms/step - loss: 3.2477 - val_loss: 3.2449\n",
            "Epoch 14/20\n",
            "269/269 [==============================] - 203s 755ms/step - loss: 3.2476 - val_loss: 3.2448\n",
            "Epoch 15/20\n",
            "269/269 [==============================] - 203s 754ms/step - loss: 3.2474 - val_loss: 3.2448\n",
            "Epoch 16/20\n",
            "269/269 [==============================] - 203s 754ms/step - loss: 3.2473 - val_loss: 3.2446\n",
            "Epoch 17/20\n",
            "269/269 [==============================] - 205s 763ms/step - loss: 3.2471 - val_loss: 3.2447\n",
            "Epoch 18/20\n",
            "269/269 [==============================] - 204s 757ms/step - loss: 3.2470 - val_loss: 3.2447\n",
            "Epoch 19/20\n",
            "269/269 [==============================] - 202s 752ms/step - loss: 3.2471 - val_loss: 3.2448\n",
            "Epoch 20/20\n",
            "269/269 [==============================] - 203s 753ms/step - loss: 3.2470 - val_loss: 3.2446\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f682644f050>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DpC9zlzMcXp"
      },
      "source": [
        "## <font color='blue'>**Inference**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5NhESYyMW_t"
      },
      "source": [
        "<font color='blue'>**Plot attention weights**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwN8tqHJP32H"
      },
      "source": [
        "import matplotlib.ticker as ticker\n",
        "\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  sentence = [\"<start>\"]+sentence.split()+[\"<end>\"]\n",
        "  predicted_sentence = predicted_sentence.split()[:-1] + ['<end>']\n",
        "  fig = plt.figure(figsize=(7, 7))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "  attention = attention[:len(predicted_sentence), :len(sentence)]\n",
        "  ax.matshow(attention, cmap='viridis', vmin=0.0)\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  ax.set_xlabel('Input text')\n",
        "  ax.set_ylabel('Output text')\n",
        "  plt.suptitle('Attention weights')"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1IhdBrgQYJr"
      },
      "source": [
        "<font color='blue'>**Predict the sentence translation**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP3kLZoPMvSu"
      },
      "source": [
        "def predict(input_sentence):\n",
        "  '''\n",
        "  A. Given input sentence, convert the sentence into integers using tokenizer used earlier\n",
        "  B. Pass the input_sequence to encoder. we get encoder_outputs, last time step hidden and cell state\n",
        "  C. Initialize index of <start> as input to decoder. and encoder final states as input_states to onestepdecoder.\n",
        "  D. till we reach max_length of decoder or till the model predicted word <end>:\n",
        "         predictions, input_states, attention_weights = model.layers[1].onestepdecoder(input_to_decoder, encoder_output, input_states)\n",
        "         Save the attention weights\n",
        "         And get the word using the tokenizer(word index) and then store it in a string.\n",
        "  E. Call plot_attention(#params)\n",
        "  F. Return the predicted sentence\n",
        "  '''\n",
        "  input_length = 20\n",
        "  lstm_units = 32\n",
        "  batch_size = 1\n",
        "\n",
        "  encoder_seq = tknizer_ita.texts_to_sequences([input_sentence]) # need to pass list of values\n",
        "  encoder_seq = pad_sequences(encoder_seq, maxlen=input_length, dtype='int32', padding='post')\n",
        "  encoder_output, state_h, state_c = model.layers[0](encoder_seq, model.layers[0].initialize_states(batch_size))\n",
        "\n",
        "  cur_vec = np.ones((1,1))\n",
        "  cur_vec[0,0] = tknizer_eng.word_index['<start>']\n",
        "  result_sentence = \"\"\n",
        "  weights_arr = []\n",
        "\n",
        "  for i in range(20):\n",
        "    predictions,dec_state_h,dec_state_c,attention_weights,context_vector = model.layers[1].onestepdecoder(cur_vec, encoder_output, state_h, state_c)\n",
        "    cur_vec = np.reshape(np.argmax(predictions),(1,1))\n",
        "    state_h = dec_state_h\n",
        "    state_c = dec_state_c\n",
        "    index= np.argmax(predictions)\n",
        "    weights_arr.append(attention_weights.numpy())\n",
        "    \n",
        "    if i==0:\n",
        "      print(index)\n",
        "      continue\n",
        "\n",
        "    if tknizer_eng.index_word[index] == \"<end>\":\n",
        "      return result_sentence\n",
        "    result_sentence += tknizer_eng.index_word[index] + \" \"\n",
        "    \n",
        "\n",
        "  return result_sentence,np.array(weights_arr)\n"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "5KmDDYkaB12z",
        "outputId": "653b8934-a18e-437f-b34e-659ed4594a27"
      },
      "source": [
        "ita_sentence = \"la radio non funziona\"\n",
        "res, att = predict(ita_sentence)\n",
        "att = np.sum(att, axis=-1)\n",
        "res"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'caption hungover net oppose roast costume glanced bullied seat pigged amendment alternatives physician skinning hurts imitate gauge mats ducks '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4sSCCyoYFjZ",
        "outputId": "b6bd667a-41d0-4d4c-f737-e866a9b1dc28"
      },
      "source": [
        "print(att[0])\n",
        "print(att[1])\n",
        "print(att[2])"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.4874569e-04 1.5869090e-01 3.9412501e-01 4.4693533e-01 1.7963686e-13\n",
            " 1.7963686e-13 1.7963686e-13 1.7963686e-13 1.7963686e-13 1.7963686e-13\n",
            " 1.7963686e-13 1.7963686e-13 1.7963686e-13 1.7963686e-13 1.7963686e-13\n",
            " 1.7963686e-13 1.7963686e-13 1.7963686e-13 1.7963686e-13 1.7963686e-13]\n",
            "[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n",
            " 0.05 0.05 0.05 0.05 0.05 0.05]\n",
            "[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n",
            " 0.05 0.05 0.05 0.05 0.05 0.05]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "mAMNgoWqK4gu",
        "outputId": "4e9c8e10-b025-44fa-9185-d457d5e74af4"
      },
      "source": [
        "plot_attention(att, ita_sentence, res)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVkHhcWPCAYp"
      },
      "source": [
        "# def plot_attention(inp_sentence, att):\n",
        "#   att = np.sum(att, axis=-1)\n",
        "#   # plt.figure(figsize=(8,3))\n",
        "#   # y_ticks = ita_sentence.split()\n",
        "#   # len_sen = len(y_ticks)\n",
        "#   # g = sns.barplot(y=list(range(len_sen)),x=att[:len_sen],palette=\"rocket_r\",orient='h')\n",
        "#   # g.set_yticklabels(y_ticks,rotation=0)\n",
        "#   # plt.title(\"Attention Weights Plot for :\"+ita_sentence)\n",
        "#   # plt.show()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmxIVOOQPWMu"
      },
      "source": [
        "<font color='blue'>**Calculate BLEU score**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjr8pRvVQjXv"
      },
      "source": [
        "def aveg_bleu_scores(test_data,n):\n",
        "  sample_list = random.sample(range(len(test_data)),n)\n",
        "  average_bleu = 0\n",
        "  pred_data = []\n",
        "  individual_bleu = []\n",
        "\n",
        "  for i in sample_list:\n",
        "    test_sentence, true_sentence = test_data.iloc[i,[0,1]]\n",
        "    pred_sentence, att = predict(test_sentence)\n",
        "    bleu_score = bleu.sentence_bleu([true_sentence.split()],pred_sentence.split())\n",
        "    average_bleu += bleu_score\n",
        "\n",
        "    # pred_data.append((true_sentence, pred_sentence))\n",
        "    individual_bleu.append(bleu_score)\n",
        "  \n",
        "  return average_bleu/n, individual_bleu"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iHiLdROM23l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "10cc55c0-dcc1-47ac-91dd-7da2569595df"
      },
      "source": [
        "#Create an object of your custom model.\n",
        "#Compile and train your model on dot scoring function.\n",
        "# Visualize few sentences randomly in Test data\n",
        "# Predict on 1000 random sentences on test data and calculate the average BLEU score of these sentences.\n",
        "# https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n",
        "import nltk.translate.bleu_score as bleu\n",
        "import random\n",
        "\n",
        "test_df = validation.copy()\n",
        "test_df = test_df[[\"italian\",\"english_out\"]]\n",
        "test_df[\"english_out\"] = test_df[\"english_out\"].apply(lambda x: x.split(\"<end>\")[0])\n",
        "\n",
        "avg_score, individual_score = aveg_bleu_scores(test_df,50)\n",
        "print(avg_score)\n",
        "print(individual_score)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-1511c45eb06d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"english_out\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"english_out\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<end>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mavg_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindividual_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maveg_bleu_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindividual_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-77-5be946922538>\u001b[0m in \u001b[0;36maveg_bleu_scores\u001b[0;34m(test_data, n)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtest_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mpred_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mbleu_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbleu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrue_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0maverage_bleu\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbleu_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWg2ferDQvT3"
      },
      "source": [
        "## Model Training with General"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rh9_w79M5JO"
      },
      "source": [
        "#Compile and train your model on general scoring function.\n",
        "# Visualize few sentences randomly in Test data\n",
        "# Predict on 1000 random sentences on test data and calculate the average BLEU score of these sentences.\n",
        "# https://www.nltk.org/_modules/nltk/translate/bleu_score.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB1jRUqZQ9AM"
      },
      "source": [
        "<font color='blue'>**Repeat the same steps for Concat scoring function**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kN9ZWViQNMB"
      },
      "source": [
        "#Compile and train your model on concat scoring function.\n",
        "# Visualize few sentences randomly in Test data\n",
        "# Predict on 1000 random sentences on test data and calculate the average BLEU score of these sentences.\n",
        "# https://www.nltk.org/_modules/nltk/translate/bleu_score.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ff1lV0ITM6_p"
      },
      "source": [
        "# Write your observations on each of the scoring function"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}