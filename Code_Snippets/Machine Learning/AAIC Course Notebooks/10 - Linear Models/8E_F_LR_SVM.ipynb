{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "8E&F_LR_SVM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HExLQrE4ZxR"
      },
      "source": [
        "<h1><font color='blue'> 8E and 8F: Finding the Probability P(Y==1|X)</font></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LuKrFzC4ZxV"
      },
      "source": [
        "<h2><font color='Geen'> 8E: Implementing Decision Function of SVM RBF Kernel</font></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wES-wWN4ZxX"
      },
      "source": [
        "<font face=' Comic Sans MS' size=3>After we train a kernel SVM model, we will be getting support vectors and their corresponsing coefficients $\\alpha_{i}$\n",
        "\n",
        "Check the documentation for better understanding of these attributes: \n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "<img src='https://i.imgur.com/K11msU4.png' width=500>\n",
        "\n",
        "As a part of this assignment you will be implementing the ```decision_function()``` of kernel SVM, here decision_function() means based on the value return by ```decision_function()``` model will classify the data point either as positive or negative\n",
        "\n",
        "Ex 1: In logistic regression After traning the models with the optimal weights $w$ we get, we will find the value $\\frac{1}{1+\\exp(-(wx+b))}$, if this value comes out to be < 0.5 we will mark it as negative class, else its positive class\n",
        "\n",
        "Ex 2: In Linear SVM After traning the models with the optimal weights $w$ we get, we will find the value of $sign(wx+b)$, if this value comes out to be -ve we will mark it as negative class, else its positive class.\n",
        "\n",
        "Similarly in Kernel SVM After traning the models with the coefficients $\\alpha_{i}$ we get, we will find the value of \n",
        "$sign(\\sum_{i=1}^{n}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here $K(x_{i},x_{q})$ is the RBF kernel. If this value comes out to be -ve we will mark $x_{q}$ as negative class, else its positive class.\n",
        "\n",
        "RBF kernel is defined as: $K(x_{i},x_{q})$ = $exp(-\\gamma ||x_{i} - x_{q}||^2)$\n",
        "\n",
        "For better understanding check this link: https://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z830CfMk4Zxa"
      },
      "source": [
        "## Task E"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuBxHiCQ4Zxc"
      },
      "source": [
        "> 1. Split the data into $X_{train}$(60), $X_{cv}$(20), $X_{test}$(20)\n",
        "\n",
        "> 2. Train $SVC(gamma=0.001, C=100.)$ on the ($X_{train}$, $y_{train}$)\n",
        "\n",
        "> 3. Get the decision boundry values $f_{cv}$ on the $X_{cv}$ data  i.e. ` `$f_{cv}$ ```= decision_function(```$X_{cv}$```)```  <font color='red'>you need to implement this decision_function()</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCgMNEvI4Zxf"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANUNIqCe4Zxn"
      },
      "source": [
        "X, y = make_classification(n_samples=5000, n_features=5, n_redundant=2,\n",
        "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHie1zqH4Zxt"
      },
      "source": [
        "### Pseudo code\n",
        "\n",
        "clf = SVC(gamma=0.001, C=100.)<br>\n",
        "clf.fit(Xtrain, ytrain)\n",
        "\n",
        "<font color='green'>def</font> <font color='blue'>decision_function</font>(Xcv, ...): #use appropriate parameters <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='green'>for</font> a data point $x_q$ <font color='green'>in</font> Xcv: <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='grey'>#write code to implement $(\\sum_{i=1}^{\\text{all the support vectors}}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here the values $y_i$, $\\alpha_{i}$, and $intercept$ can be obtained from the trained model</font><br>\n",
        "   <font color='green'>return</font> <font color='grey'><i># the decision_function output for all the data points in the Xcv</i></font>\n",
        "    \n",
        "fcv = decision_function(Xcv, ...)  <i># based on your requirement you can pass any other parameters </i>\n",
        "\n",
        "<b>Note</b>: Make sure the values you get as fcv, should be equal to outputs of clf.decision_function(Xcv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC2Aq5ysQiSu"
      },
      "source": [
        "# Task 8E"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbXG26WrQgIm"
      },
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.4,random_state=0)\r\n",
        "X_cv,X_test,y_cv,y_test = train_test_split(X_test,y_test,test_size=0.5,random_state=0)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTGW7ye7RPOx",
        "outputId": "3ac2131d-9fa5-441d-be27-2ae3d502deaa"
      },
      "source": [
        "# SVC\r\n",
        "clf = SVC(gamma=0.001, C=100)\r\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=100, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojOkqP_cSsw9"
      },
      "source": [
        "def rbf_kernel(x1,x2,gamma):\r\n",
        "  euclid_d = sum([abs(x-y)**2 for x,y in zip(x1,x2)])\r\n",
        "  return np.exp(-1*gamma*euclid_d)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcFCrATsSdrL"
      },
      "source": [
        "def decision_function(Xcv,dual_coefs,support_vec,intercept,gamma):\r\n",
        "  n = len(support_vec)\r\n",
        "  y_res = []\r\n",
        "  for x in Xcv:\r\n",
        "    res = 0\r\n",
        "    for i in range(n):\r\n",
        "      res += dual_coefs[i][0]*rbf_kernel(support_vec[i],x,gamma)\r\n",
        "    y_res.append(res+intercept)\r\n",
        "  return y_res"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoxFIG6ARPLt"
      },
      "source": [
        "coefs = np.reshape(clf.dual_coef_,(-1,1))\r\n",
        "intercept = clf.intercept_\r\n",
        "svcs = clf.support_vectors_\r\n",
        "\r\n",
        "f_cv = decision_function(X_cv,coefs,svcs,intercept,0.001)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR564AFicM9O"
      },
      "source": [
        "## Compare with sklearn implementation of decision_function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEAMx8O6cMje"
      },
      "source": [
        "f_cv_sklearn = clf.decision_function(X_cv)\r\n",
        "\r\n",
        "def print_i_ele(idx):\r\n",
        "  print(\"Sklearn f_cv for %d datapoint: %f\" % (idx, f_cv_sklearn[idx]))\r\n",
        "  print(\"Custom Implemented f_cv for %d datapoint: %f\" % (idx, f_cv[idx]))\r\n",
        "  print()\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dNxVjIcHBgz",
        "outputId": "1c3d2275-45c4-4265-e4e6-39f285158993"
      },
      "source": [
        "print_i_ele(0)\r\n",
        "print_i_ele(845)\r\n",
        "print_i_ele(10)\r\n",
        "print_i_ele(100)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sklearn f_cv for 0 datapoint: -1.387457\n",
            "Custom Implemented f_cv for 0 datapoint: -1.387457\n",
            "\n",
            "Sklearn f_cv for 845 datapoint: -3.304218\n",
            "Custom Implemented f_cv for 845 datapoint: -3.304218\n",
            "\n",
            "Sklearn f_cv for 10 datapoint: 1.751849\n",
            "Custom Implemented f_cv for 10 datapoint: 1.751849\n",
            "\n",
            "Sklearn f_cv for 100 datapoint: -3.703750\n",
            "Custom Implemented f_cv for 100 datapoint: -3.703750\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0bKCboN4Zxu"
      },
      "source": [
        "<h2><font color='Geen'> 8F: Implementing Platt Scaling to find P(Y==1|X)</font></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMn7OEN94Zxw"
      },
      "source": [
        "Check this <a href='https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a'>PDF</a>\n",
        "<img src='https://i.imgur.com/CAMnVnh.png'>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0n5EFkx4Zxz"
      },
      "source": [
        "## TASK F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0HOqVJq4Zx1"
      },
      "source": [
        "\n",
        "> 4. Apply SGD algorithm with ($f_{cv}$, $y_{cv}$) and find the weight $W$ intercept $b$ ```Note: here our data is of one dimensional so we will have a one dimensional weight vector i.e W.shape (1,)``` \n",
        "\n",
        "> Note1: Don't forget to change the values of $y_{cv}$ as mentioned in the above image. you will calculate y+, y- based on data points in train data\n",
        "\n",
        "> Note2: the Sklearn's SGD algorithm doesn't support the real valued outputs, you need to use the code that was done in the `'Logistic Regression with SGD and L2'` Assignment after modifying loss function, and use same parameters that used in that assignment.\n",
        "<img src='https://i.imgur.com/zKYE9Oc.png'>\n",
        "if Y[i] is 1, it will be replaced with y+ value else it will replaced with y- value\n",
        "\n",
        "> 5. For a given data point from $X_{test}$, $P(Y=1|X) = \\frac{1}{1+exp(-(W*f_{test}+ b))}$ where ` `$f_{test}$ ```= decision_function(```$X_{test}$```)```, W and b will be learned as metioned in the above step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTY7z2bd4Zx2"
      },
      "source": [
        "__Note: in the above algorithm, the steps 2, 4 might need hyper parameter tuning, To reduce the complexity of the assignment we are excluding the hyerparameter tuning part, but intrested students can try that__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM3odN1Z4Zx3"
      },
      "source": [
        "\n",
        "If any one wants to try other calibration algorithm istonic regression also please check these tutorials\n",
        "\n",
        "1. http://fa.bianp.net/blog/tag/scikit-learn.html#fn:1\n",
        "\n",
        "2. https://drive.google.com/open?id=1MzmA7QaP58RDzocB0RBmRiWfl7Co_VJ7\n",
        "\n",
        "3. https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a\n",
        "\n",
        "4. https://stat.fandom.com/wiki/Isotonic_regression#Pool_Adjacent_Violators_Algorithm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WavjhyPwbsBX"
      },
      "source": [
        "## SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4VQ2T_ebqOk"
      },
      "source": [
        "def sigmoid(z):\r\n",
        "    return 1/(1+np.exp(-z))\r\n",
        "\r\n",
        "def gradient_dw(x,y,w,b,alpha,N):\r\n",
        "    wtx = np.dot(w,x)\r\n",
        "    term_1 = x*(y-sigmoid(wtx+b))\r\n",
        "    term_2 = (alpha/N)*w\r\n",
        "    dw = term_1-term_2\r\n",
        "    return dw\r\n",
        "\r\n",
        "def gradient_db(x,y,w,b):\r\n",
        "    db = y - sigmoid(np.dot(w,x)+b)\r\n",
        "    return db\r\n",
        "\r\n",
        "def predict(w,b, X):\r\n",
        "  pred = []\r\n",
        "  for x in X:\r\n",
        "    res = np.dot(w.T,x)+b\r\n",
        "    pred.append(sigmoid(res))\r\n",
        "  return pred\r\n",
        "\r\n",
        "\r\n",
        "def logloss(y_true,y_pred):\r\n",
        "    n = len(y_true)\r\n",
        "    loss = 0\r\n",
        "    #read each pair of (y,y_score)\r\n",
        "    for i in range(n):\r\n",
        "      y = y_true[i]\r\n",
        "      y_score = y_pred[i]\r\n",
        "      loss += (y*math.log10(y_score))+((1-y)*(math.log10(1-y_score)))\r\n",
        "    \r\n",
        "    loss *= (-1)/n\r\n",
        "    return loss\r\n",
        "    \r\n",
        "def initialize_weights(dim):\r\n",
        "    w = np.zeros(len(dim))\r\n",
        "    b = 0\r\n",
        "\r\n",
        "    return w,b"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcg8zfhKcY_r"
      },
      "source": [
        "from tqdm import tqdm\r\n",
        "def train(X_train,y_train,epochs,alpha,eta0):\r\n",
        "  w,b = initialize_weights(X_train[0])\r\n",
        "  N=len(X_train)\r\n",
        "\r\n",
        "  train_loss =[]\r\n",
        "\r\n",
        "  for _ in tqdm(range(epochs)):\r\n",
        "    for x,y in zip(X_train,y_train):\r\n",
        "      w_grad = gradient_dw(x,y,w,b,alpha,N)\r\n",
        "      b_grad = gradient_db(x,y,w,b)\r\n",
        "      w = w + (eta0 * w_grad)\r\n",
        "      b = b + (eta0 * b_grad)\r\n",
        "    \r\n",
        "    # LogLoss on train data\r\n",
        "    y_train_pred = predict(w,b,X_train)\r\n",
        "    log_loss_train = logloss(y_train, y_train_pred)\r\n",
        "\r\n",
        "    # save loss of epoc\r\n",
        "    train_loss.append(log_loss_train)\r\n",
        "\r\n",
        "  return (w,b,train_loss)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi3RfR3McZvZ"
      },
      "source": [
        "alpha=0.0001\r\n",
        "eta0=0.0001\r\n",
        "N=len(X_train)\r\n",
        "epochs=15\r\n",
        "# w,b,train_loss,test_loss = train(X_train,y_train,X_test,y_test,epochs,alpha,eta0)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ROTgLHIizhi"
      },
      "source": [
        "f_cv = decision_function(X_cv,coefs,svcs,intercept,0.001)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0eD2NFKi9nc"
      },
      "source": [
        "# finding y+ and y-\r\n",
        "pCount = len(np.where(y_train == 1)[0])\r\n",
        "nCount = len(np.where(y_train == 0)[0])\r\n",
        "\r\n",
        "yp = (pCount+1)/(pCount+2)\r\n",
        "yn = 1/(nCount+2)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7laFwn_ojdCL"
      },
      "source": [
        "y_cv_mod = [yp if y==1 else yn for y in y_cv]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wtKMoj-jz_e",
        "outputId": "9245f821-453b-46b5-a28a-7b89147a27c2"
      },
      "source": [
        "w,b,train_loss = train(f_cv,y_cv_mod,epochs,alpha,eta0)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:00<00:00, 49.14it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "L5e2WJ8sj1d4",
        "outputId": "042597be-2df4-42c2-929d-09f1056f5a84"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "plt.plot(train_loss)\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.ylabel(\"Loss\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyUm76qIlSRZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55c6aa17-ab03-4d76-82c7-bdf5bb41e81c"
      },
      "source": [
        "print(w)\r\n",
        "print(b)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.71409206]\n",
            "-0.10985288006827136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4uBBmDtVMKN"
      },
      "source": [
        "# find P\r\n",
        "def find_p(X_point, w,b):\r\n",
        "  f_test = decision_function([X_point],coefs,svcs,intercept,0.001)\r\n",
        "  return 1 / (1+np.exp(-(w*f_test+b)))[0][0]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkNYbwMsKY8w",
        "outputId": "b69d7d50-7ae4-42a2-fa28-450498482a6d"
      },
      "source": [
        "p_vals = [find_p(x,w,b) for x in X_test]\r\n",
        "p_vals"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.795783819146929,\n",
              " 0.03935535371737212,\n",
              " 0.11223979312108773,\n",
              " 0.13211597474268041,\n",
              " 0.762037840512687,\n",
              " 0.045628986982347734,\n",
              " 0.08203450071364381,\n",
              " 0.10721001124259123,\n",
              " 0.14750139467834777,\n",
              " 0.44766508734750793,\n",
              " 0.12349451817102308,\n",
              " 0.5841844212624675,\n",
              " 0.1524829476485025,\n",
              " 0.6737699081052851,\n",
              " 0.0906870868420059,\n",
              " 0.2654310532328188,\n",
              " 0.7191684001308745,\n",
              " 0.12881817877484647,\n",
              " 0.749220490714279,\n",
              " 0.32943904213069325,\n",
              " 0.021101512857306104,\n",
              " 0.1134337287738212,\n",
              " 0.28810047673044936,\n",
              " 0.07923321327093828,\n",
              " 0.05964402376197869,\n",
              " 0.20127877569376754,\n",
              " 0.1865294150541119,\n",
              " 0.33628128706223054,\n",
              " 0.08134041097960847,\n",
              " 0.14919270908454113,\n",
              " 0.7731208077535354,\n",
              " 0.6384993790524338,\n",
              " 0.10041366766794253,\n",
              " 0.6994579909433971,\n",
              " 0.19051067181755743,\n",
              " 0.5751626729414853,\n",
              " 0.29926646174759286,\n",
              " 0.11396245242031672,\n",
              " 0.19686611911012472,\n",
              " 0.12711915043174374,\n",
              " 0.6115447728652709,\n",
              " 0.041801821904530134,\n",
              " 0.7602242867999255,\n",
              " 0.07256070732202438,\n",
              " 0.1876701781722857,\n",
              " 0.13981156300059708,\n",
              " 0.5351044542643643,\n",
              " 0.31295468836503576,\n",
              " 0.7215927719315346,\n",
              " 0.0875823417669238,\n",
              " 0.13161717011218022,\n",
              " 0.8278173460981284,\n",
              " 0.41378890759507775,\n",
              " 0.07551176840603643,\n",
              " 0.721794515265492,\n",
              " 0.18712088680937095,\n",
              " 0.2626021021524251,\n",
              " 0.7380725675057251,\n",
              " 0.1828066500434953,\n",
              " 0.24478335945789723,\n",
              " 0.7769014869325626,\n",
              " 0.6718955296151501,\n",
              " 0.17191822677456164,\n",
              " 0.7636543403216495,\n",
              " 0.6902888590273996,\n",
              " 0.08616695860041197,\n",
              " 0.12668644386853992,\n",
              " 0.49227621398204086,\n",
              " 0.1077805079540345,\n",
              " 0.05865579438753139,\n",
              " 0.3415158895381001,\n",
              " 0.07298591840739489,\n",
              " 0.715290994829443,\n",
              " 0.06813814605459503,\n",
              " 0.10897782089537598,\n",
              " 0.05925552847795417,\n",
              " 0.08083554567246683,\n",
              " 0.13161790963404973,\n",
              " 0.2454619318617697,\n",
              " 0.14526494417380417,\n",
              " 0.7570111195114018,\n",
              " 0.07686526222825757,\n",
              " 0.3312087924537983,\n",
              " 0.7634644662513225,\n",
              " 0.12364446284445697,\n",
              " 0.8334872036225192,\n",
              " 0.17382956073625755,\n",
              " 0.7708270591309453,\n",
              " 0.8182572957013733,\n",
              " 0.5174847063176966,\n",
              " 0.11475850920385358,\n",
              " 0.10586795855643476,\n",
              " 0.13081085039175547,\n",
              " 0.7359278309201048,\n",
              " 0.1447908591746739,\n",
              " 0.5577891560811324,\n",
              " 0.12818571335144455,\n",
              " 0.24146504797688334,\n",
              " 0.4514691824355282,\n",
              " 0.5069580535332068,\n",
              " 0.08820602213672551,\n",
              " 0.0509465737641136,\n",
              " 0.5393620176827445,\n",
              " 0.1041565397456771,\n",
              " 0.6627723284646745,\n",
              " 0.4068722547306494,\n",
              " 0.10308834128096404,\n",
              " 0.3277884825110006,\n",
              " 0.0597046462726348,\n",
              " 0.10562548170686079,\n",
              " 0.15785764463394833,\n",
              " 0.4497628934004366,\n",
              " 0.10552967991673055,\n",
              " 0.1462855254940503,\n",
              " 0.05072702252612073,\n",
              " 0.7195969259200669,\n",
              " 0.1312422367547631,\n",
              " 0.6631904375427226,\n",
              " 0.7430795776791543,\n",
              " 0.538053656434005,\n",
              " 0.13747980144673455,\n",
              " 0.27213204239821565,\n",
              " 0.765010952324188,\n",
              " 0.08042749250639729,\n",
              " 0.12250774380075068,\n",
              " 0.05581388639332212,\n",
              " 0.11837496021662504,\n",
              " 0.7243805169511436,\n",
              " 0.16098099437514785,\n",
              " 0.09128308046735395,\n",
              " 0.5848456746512287,\n",
              " 0.7794034519288205,\n",
              " 0.05331839374316326,\n",
              " 0.7616309074218016,\n",
              " 0.7598874353952682,\n",
              " 0.049434044415652696,\n",
              " 0.2306842768745667,\n",
              " 0.7494916964426571,\n",
              " 0.0496662531927069,\n",
              " 0.30566432458616605,\n",
              " 0.342373545117688,\n",
              " 0.19186684734109205,\n",
              " 0.20995872273426178,\n",
              " 0.2427998285719168,\n",
              " 0.44793834770789454,\n",
              " 0.7536343968311959,\n",
              " 0.2303357765104639,\n",
              " 0.10637281057848094,\n",
              " 0.10377476460252545,\n",
              " 0.08991006266011516,\n",
              " 0.2115271533654242,\n",
              " 0.12736198756317865,\n",
              " 0.06181365113929063,\n",
              " 0.21706917760903413,\n",
              " 0.11188579817109384,\n",
              " 0.15239212467968802,\n",
              " 0.24062580048849824,\n",
              " 0.05564569408226412,\n",
              " 0.09994408547350885,\n",
              " 0.1553679179400689,\n",
              " 0.4150345924193956,\n",
              " 0.10186790423661464,\n",
              " 0.08572276608886259,\n",
              " 0.05428894657349493,\n",
              " 0.06969615862088026,\n",
              " 0.07398135061826255,\n",
              " 0.22928575555733946,\n",
              " 0.11113828663062342,\n",
              " 0.5563999547086368,\n",
              " 0.47530041718561616,\n",
              " 0.15800160030344285,\n",
              " 0.1574430645379417,\n",
              " 0.07645018216045002,\n",
              " 0.6271231628439415,\n",
              " 0.1356172340958396,\n",
              " 0.19090070055817868,\n",
              " 0.06256875630681782,\n",
              " 0.030964426051528392,\n",
              " 0.08916180150253664,\n",
              " 0.06296198749703591,\n",
              " 0.3950532619535175,\n",
              " 0.31935210039647,\n",
              " 0.08920195715284585,\n",
              " 0.39748773160005757,\n",
              " 0.08239006955045408,\n",
              " 0.18149577578176238,\n",
              " 0.09368516347995354,\n",
              " 0.6754496789152522,\n",
              " 0.3935383891960853,\n",
              " 0.8201131782814541,\n",
              " 0.1182281190819303,\n",
              " 0.11474441498708994,\n",
              " 0.10671057060971376,\n",
              " 0.088229162740659,\n",
              " 0.06930225091595404,\n",
              " 0.7269717046125104,\n",
              " 0.6916914879285008,\n",
              " 0.6920757042026723,\n",
              " 0.8265966319215886,\n",
              " 0.7622219245710137,\n",
              " 0.6023609671101754,\n",
              " 0.7762554973315012,\n",
              " 0.07719383963854383,\n",
              " 0.20130859328997044,\n",
              " 0.16199835365659118,\n",
              " 0.06842052100826485,\n",
              " 0.15259289601553744,\n",
              " 0.07290536732004456,\n",
              " 0.1893759404553749,\n",
              " 0.21481221146940507,\n",
              " 0.2888855950755743,\n",
              " 0.3735501386980064,\n",
              " 0.16159184318434394,\n",
              " 0.14217821842114733,\n",
              " 0.8292831789667813,\n",
              " 0.05386039818358199,\n",
              " 0.21553182331140458,\n",
              " 0.6524632743962989,\n",
              " 0.6994393709585004,\n",
              " 0.25749596786884316,\n",
              " 0.14005594520924444,\n",
              " 0.11329255715339111,\n",
              " 0.199622356302157,\n",
              " 0.30782762909367,\n",
              " 0.14102718883826465,\n",
              " 0.11081376420043741,\n",
              " 0.1251424133030954,\n",
              " 0.2951227262629841,\n",
              " 0.10755562858844066,\n",
              " 0.18342643341985254,\n",
              " 0.21175332929175794,\n",
              " 0.17047953589833073,\n",
              " 0.1577265733318811,\n",
              " 0.5279621950081214,\n",
              " 0.08314052953165268,\n",
              " 0.1832718825193577,\n",
              " 0.7959772830742186,\n",
              " 0.8014622838380102,\n",
              " 0.6844306674000208,\n",
              " 0.16520871867600823,\n",
              " 0.03919569961697069,\n",
              " 0.0859846918113587,\n",
              " 0.10176212631077708,\n",
              " 0.3106935003271112,\n",
              " 0.6144694069435073,\n",
              " 0.1512689922023774,\n",
              " 0.12701543560334805,\n",
              " 0.7465402729567614,\n",
              " 0.08175621250115643,\n",
              " 0.16795512851945177,\n",
              " 0.5540840377824817,\n",
              " 0.1486017601249728,\n",
              " 0.740337698199296,\n",
              " 0.1507009664832272,\n",
              " 0.06004323695628989,\n",
              " 0.7981832951560339,\n",
              " 0.6253997260063023,\n",
              " 0.05549223450317897,\n",
              " 0.7547566720610599,\n",
              " 0.7269372864532285,\n",
              " 0.06636111142271746,\n",
              " 0.11661282192045973,\n",
              " 0.10006459619820546,\n",
              " 0.5561958566298144,\n",
              " 0.8072053796530627,\n",
              " 0.401119181988072,\n",
              " 0.16219681657861776,\n",
              " 0.776848557375985,\n",
              " 0.1585673618051853,\n",
              " 0.8245668230281791,\n",
              " 0.10082520454828284,\n",
              " 0.13991766208612794,\n",
              " 0.6629167925773928,\n",
              " 0.0848016118590304,\n",
              " 0.8258691102691537,\n",
              " 0.05514446618993308,\n",
              " 0.7965243840829608,\n",
              " 0.181552385622978,\n",
              " 0.18720687298504562,\n",
              " 0.8289315848887585,\n",
              " 0.7567001183912991,\n",
              " 0.1171073100053097,\n",
              " 0.819076224666005,\n",
              " 0.10281941354301827,\n",
              " 0.05423484868551151,\n",
              " 0.34280969633341396,\n",
              " 0.7680075864369459,\n",
              " 0.7555388753888728,\n",
              " 0.4115504689051674,\n",
              " 0.5730965388063289,\n",
              " 0.556574634365649,\n",
              " 0.15730968763661005,\n",
              " 0.059377880026332645,\n",
              " 0.54002930941173,\n",
              " 0.2695132790735328,\n",
              " 0.48905470918497423,\n",
              " 0.1107184385210793,\n",
              " 0.6178099089153833,\n",
              " 0.1306510256280165,\n",
              " 0.1647520389094384,\n",
              " 0.08817491269990259,\n",
              " 0.877820280062675,\n",
              " 0.6863346421174208,\n",
              " 0.16513248667277183,\n",
              " 0.06873403575529513,\n",
              " 0.05093897328954146,\n",
              " 0.13195083451167086,\n",
              " 0.5272291582004077,\n",
              " 0.8073075772966481,\n",
              " 0.22663602213264644,\n",
              " 0.13543553941702038,\n",
              " 0.4290782560243063,\n",
              " 0.36145857874269555,\n",
              " 0.37582192209183196,\n",
              " 0.1487795612556874,\n",
              " 0.12219451029735036,\n",
              " 0.2805747500221665,\n",
              " 0.12634183869808213,\n",
              " 0.650428806685771,\n",
              " 0.738807981645929,\n",
              " 0.3816926366670368,\n",
              " 0.7960354883480683,\n",
              " 0.7648366743162327,\n",
              " 0.049070036646197635,\n",
              " 0.16285292508601024,\n",
              " 0.39630155507328113,\n",
              " 0.15352179535755156,\n",
              " 0.09615231806342149,\n",
              " 0.4084937546595731,\n",
              " 0.7193064589957743,\n",
              " 0.03432495327519718,\n",
              " 0.07806934845971422,\n",
              " 0.4908805348078671,\n",
              " 0.2805480186692478,\n",
              " 0.1708671629012431,\n",
              " 0.10912077813218647,\n",
              " 0.13990305720483096,\n",
              " 0.6996336521120512,\n",
              " 0.5391044548485521,\n",
              " 0.8751360377782985,\n",
              " 0.2434747424264853,\n",
              " 0.09437850706177084,\n",
              " 0.5398252469920752,\n",
              " 0.7728618934126068,\n",
              " 0.05701239850093324,\n",
              " 0.03948155412688114,\n",
              " 0.29955393582780865,\n",
              " 0.7257624014247145,\n",
              " 0.09970625411526897,\n",
              " 0.19558550057490173,\n",
              " 0.20060213021034642,\n",
              " 0.14033309124772572,\n",
              " 0.07091777345790852,\n",
              " 0.08582621561825847,\n",
              " 0.22962992596598022,\n",
              " 0.10061220380655361,\n",
              " 0.06353545625214463,\n",
              " 0.34528661186871507,\n",
              " 0.47716452372644236,\n",
              " 0.7861208714638528,\n",
              " 0.27156479863757155,\n",
              " 0.041512698216427515,\n",
              " 0.13816176394047755,\n",
              " 0.8505231909937938,\n",
              " 0.8539437565238921,\n",
              " 0.12553049440830563,\n",
              " 0.09922216580628637,\n",
              " 0.9018741404926246,\n",
              " 0.2180918895315719,\n",
              " 0.18237834450837143,\n",
              " 0.31221730709465306,\n",
              " 0.10533795443212289,\n",
              " 0.1021000595751028,\n",
              " 0.5597337706207673,\n",
              " 0.6479817048539928,\n",
              " 0.19088862775432372,\n",
              " 0.13780078331584242,\n",
              " 0.7329605815014434,\n",
              " 0.23025635367254899,\n",
              " 0.17820089947928822,\n",
              " 0.5062824851195198,\n",
              " 0.40090640611741996,\n",
              " 0.14107681817619608,\n",
              " 0.02901408470859467,\n",
              " 0.7683496871812632,\n",
              " 0.1821353674410772,\n",
              " 0.5482803384677388,\n",
              " 0.05277568617116641,\n",
              " 0.05024426382729702,\n",
              " 0.21629668358444495,\n",
              " 0.5734914290315891,\n",
              " 0.8071961135503443,\n",
              " 0.7164402158406902,\n",
              " 0.722344886322892,\n",
              " 0.20396264675119327,\n",
              " 0.19386426174897398,\n",
              " 0.16871482176506775,\n",
              " 0.16370923250286062,\n",
              " 0.11485076041726458,\n",
              " 0.1196439077705042,\n",
              " 0.8418883693592845,\n",
              " 0.17012363283522708,\n",
              " 0.7659872627329019,\n",
              " 0.6918350217427193,\n",
              " 0.6841725578106084,\n",
              " 0.1385780566414969,\n",
              " 0.49320351674152185,\n",
              " 0.10228734779661972,\n",
              " 0.25448846215923127,\n",
              " 0.08591854774394041,\n",
              " 0.1831571879730502,\n",
              " 0.21198718456050805,\n",
              " 0.2389640026500711,\n",
              " 0.48035721422670813,\n",
              " 0.1328712058691889,\n",
              " 0.0528873113996526,\n",
              " 0.05025535081719436,\n",
              " 0.2671133360691229,\n",
              " 0.827924314747306,\n",
              " 0.15995062764618415,\n",
              " 0.6187076331815784,\n",
              " 0.04558770162058152,\n",
              " 0.10943249102223748,\n",
              " 0.34619876880420936,\n",
              " 0.7337488271942144,\n",
              " 0.11048241215479401,\n",
              " 0.8004640331249412,\n",
              " 0.733149865805288,\n",
              " 0.7409381975387586,\n",
              " 0.13269103369923418,\n",
              " 0.07221305051310735,\n",
              " 0.7882517068096093,\n",
              " 0.723467830141955,\n",
              " 0.2491550984028079,\n",
              " 0.7653527370706015,\n",
              " 0.23490174991540524,\n",
              " 0.38406397620250426,\n",
              " 0.7914551447301229,\n",
              " 0.09212796167423379,\n",
              " 0.7166585678019785,\n",
              " 0.7676099195307986,\n",
              " 0.795418461505211,\n",
              " 0.22432618481070377,\n",
              " 0.19961998411555534,\n",
              " 0.10146125035348318,\n",
              " 0.11201675563383955,\n",
              " 0.7335895303416944,\n",
              " 0.03447753896743904,\n",
              " 0.0852785585880561,\n",
              " 0.6413811672122246,\n",
              " 0.6447865933673744,\n",
              " 0.16584716462413002,\n",
              " 0.7929809113676852,\n",
              " 0.6908334855875113,\n",
              " 0.15211889154679903,\n",
              " 0.2797254688977863,\n",
              " 0.1793610495176623,\n",
              " 0.13488902493172114,\n",
              " 0.7668378818184809,\n",
              " 0.4227895654184162,\n",
              " 0.12352776074041064,\n",
              " 0.3022251061720054,\n",
              " 0.10933906005972235,\n",
              " 0.12784212865922365,\n",
              " 0.09328711110269479,\n",
              " 0.23815883685209463,\n",
              " 0.2533105724291876,\n",
              " 0.1445649910889444,\n",
              " 0.12163805030765364,\n",
              " 0.17011619232993397,\n",
              " 0.35523012861133957,\n",
              " 0.13551811017604767,\n",
              " 0.12667834855285448,\n",
              " 0.11908041968144423,\n",
              " 0.36390704456240186,\n",
              " 0.18870974006032262,\n",
              " 0.7913573275125257,\n",
              " 0.572532847977104,\n",
              " 0.15201204381907205,\n",
              " 0.7550779379771584,\n",
              " 0.37912180861575795,\n",
              " 0.08388437863144252,\n",
              " 0.578182939288538,\n",
              " 0.2284591841151371,\n",
              " 0.11953588939053061,\n",
              " 0.07276496406283167,\n",
              " 0.6634218015776348,\n",
              " 0.2459228257481457,\n",
              " 0.7078913249142605,\n",
              " 0.5650253105577157,\n",
              " 0.049451331906365546,\n",
              " 0.2827927323085907,\n",
              " 0.6225753939754428,\n",
              " 0.315108841584681,\n",
              " 0.1334179832448562,\n",
              " 0.055153691900302705,\n",
              " 0.581910372615812,\n",
              " 0.12885286330044854,\n",
              " 0.08551763556514068,\n",
              " 0.09855007587686958,\n",
              " 0.09803409594037332,\n",
              " 0.17969961398638462,\n",
              " 0.2511628109259297,\n",
              " 0.10588671500177481,\n",
              " 0.7837278914283656,\n",
              " 0.7963538156145136,\n",
              " 0.060798242930197734,\n",
              " 0.8937260711903329,\n",
              " 0.36299871848925985,\n",
              " 0.09583535933790596,\n",
              " 0.3188851745890198,\n",
              " 0.7015624333808682,\n",
              " 0.14576804262796111,\n",
              " 0.14691517840477405,\n",
              " 0.8201150545066862,\n",
              " 0.2101414732202637,\n",
              " 0.7550462973382767,\n",
              " 0.10242591758813405,\n",
              " 0.05123907965211457,\n",
              " 0.5379783778056016,\n",
              " 0.31017207712791556,\n",
              " 0.7322676735375983,\n",
              " 0.774786958653302,\n",
              " 0.11483766118225826,\n",
              " 0.1624201610229119,\n",
              " 0.07730746542062164,\n",
              " 0.2928824646491522,\n",
              " 0.7394884555799816,\n",
              " 0.7300768240935075,\n",
              " 0.11918925534913215,\n",
              " 0.09781407460946662,\n",
              " 0.12283221367257462,\n",
              " 0.16210208410919913,\n",
              " 0.08927309287341637,\n",
              " 0.6691777447868337,\n",
              " 0.7102636624144909,\n",
              " 0.1328479264770796,\n",
              " 0.5297760364701578,\n",
              " 0.7732941151065292,\n",
              " 0.03569822163653482,\n",
              " 0.780405133114552,\n",
              " 0.3103096378037878,\n",
              " 0.08146911035821679,\n",
              " 0.1202505040626024,\n",
              " 0.7405903887431798,\n",
              " 0.06863295784440808,\n",
              " 0.08528554972520294,\n",
              " 0.16253047054619768,\n",
              " 0.28999132983822207,\n",
              " 0.13503340573389547,\n",
              " 0.3351476926629581,\n",
              " 0.7163488356110229,\n",
              " 0.8881502405361646,\n",
              " 0.15946193166451392,\n",
              " 0.17718272927270923,\n",
              " 0.09297044858758334,\n",
              " 0.7851560355606082,\n",
              " 0.20120719022063807,\n",
              " 0.10511160005579431,\n",
              " 0.2412569229232158,\n",
              " 0.18396350148349913,\n",
              " 0.11674151879114149,\n",
              " 0.4076371210933022,\n",
              " 0.14418120197889245,\n",
              " 0.09256710356999123,\n",
              " 0.4986704067073752,\n",
              " 0.034952878619423665,\n",
              " 0.4883569974847402,\n",
              " 0.09629978960084357,\n",
              " 0.08812276693649901,\n",
              " 0.24261207630042178,\n",
              " 0.15821699660082275,\n",
              " 0.14147198473165598,\n",
              " 0.7196336601149533,\n",
              " 0.12372460275201931,\n",
              " 0.2707333715835499,\n",
              " 0.05566682406014789,\n",
              " 0.7424922744681798,\n",
              " 0.13935329004489053,\n",
              " 0.6965127993830412,\n",
              " 0.2943697538140661,\n",
              " 0.20113107077293726,\n",
              " 0.049629456874598375,\n",
              " 0.1672322105843329,\n",
              " 0.11218101775083845,\n",
              " 0.4579817769336219,\n",
              " 0.12988112813770306,\n",
              " 0.06640711086597316,\n",
              " 0.08659742372220243,\n",
              " 0.2154350422510055,\n",
              " 0.30851114147550196,\n",
              " 0.22116746694479947,\n",
              " 0.19014308641820699,\n",
              " 0.8502040088291618,\n",
              " 0.17385837114896924,\n",
              " 0.24050155725781025,\n",
              " 0.018229641639958778,\n",
              " 0.7526991831270319,\n",
              " 0.14378370415643285,\n",
              " 0.2064274006510031,\n",
              " 0.06489480652366067,\n",
              " 0.4598134158048723,\n",
              " 0.17556614137853782,\n",
              " 0.3223620046562762,\n",
              " 0.16772614857376217,\n",
              " 0.0662749367431411,\n",
              " 0.5884510806517828,\n",
              " 0.568348904904354,\n",
              " 0.15405346692185878,\n",
              " 0.11249263040828274,\n",
              " 0.731505998136668,\n",
              " 0.5955803905097533,\n",
              " 0.30520721599191947,\n",
              " 0.5157555525218627,\n",
              " 0.7472265021536761,\n",
              " 0.22871398002107926,\n",
              " 0.3471464972133244,\n",
              " 0.1017957148037527,\n",
              " 0.7502243848408555,\n",
              " 0.730821344831106,\n",
              " 0.16815611692092725,\n",
              " 0.39082547623023045,\n",
              " 0.188018259107205,\n",
              " 0.5095909860703336,\n",
              " 0.7127280459321095,\n",
              " 0.07814935231553773,\n",
              " 0.3403373494635237,\n",
              " 0.07240001066439125,\n",
              " 0.6020017342185883,\n",
              " 0.3424009750616882,\n",
              " 0.08326693874649063,\n",
              " 0.2100372150683684,\n",
              " 0.05173717369179092,\n",
              " 0.10411680088139318,\n",
              " 0.6604499378794658,\n",
              " 0.7537622642111064,\n",
              " 0.15805361669549958,\n",
              " 0.17435597481090523,\n",
              " 0.5651898033463116,\n",
              " 0.027079139860372702,\n",
              " 0.25557651893932554,\n",
              " 0.12160505126871678,\n",
              " 0.045451503517261224,\n",
              " 0.050586099771141244,\n",
              " 0.6943047081061413,\n",
              " 0.1631186266618596,\n",
              " 0.689102466822257,\n",
              " 0.24104394252476358,\n",
              " 0.25752017889558043,\n",
              " 0.38129591118954653,\n",
              " 0.6483283683754398,\n",
              " 0.09326715404884005,\n",
              " 0.7466829395312283,\n",
              " 0.07800489333559034,\n",
              " 0.17281990167103886,\n",
              " 0.7744600017398652,\n",
              " 0.19610080845215297,\n",
              " 0.038730020935182866,\n",
              " 0.2280405867957194,\n",
              " 0.36884404179867364,\n",
              " 0.7147171826650924,\n",
              " 0.5383694587589011,\n",
              " 0.17126103991600228,\n",
              " 0.2612956745606027,\n",
              " 0.758219563925729,\n",
              " 0.06819896884381617,\n",
              " 0.6417189481940214,\n",
              " 0.11734926602475276,\n",
              " 0.1005704754261216,\n",
              " 0.4011783475379519,\n",
              " 0.41696292826710213,\n",
              " 0.33568912934031486,\n",
              " 0.5723791471139604,\n",
              " 0.19582395133931965,\n",
              " 0.05265053216114361,\n",
              " 0.1095222634746099,\n",
              " 0.37338415519307383,\n",
              " 0.3147734687547752,\n",
              " 0.13410426630911473,\n",
              " 0.5882659604586931,\n",
              " 0.3988881357639372,\n",
              " 0.4450648602088562,\n",
              " 0.10844847448134053,\n",
              " 0.7687036458366779,\n",
              " 0.7820755211219079,\n",
              " 0.12463901252861996,\n",
              " 0.17395307214625017,\n",
              " 0.16120474087684702,\n",
              " 0.115688033819226,\n",
              " 0.31699892125309953,\n",
              " 0.14827358799637022,\n",
              " 0.7079267638645191,\n",
              " 0.15182385658412026,\n",
              " 0.2733025340557902,\n",
              " 0.1823956120094501,\n",
              " 0.11784111376626846,\n",
              " 0.31487479209788694,\n",
              " 0.1716939513899429,\n",
              " 0.08608503442282754,\n",
              " 0.5122787243108534,\n",
              " 0.6139948173973793,\n",
              " 0.49931899790711143,\n",
              " 0.08473373318825761,\n",
              " 0.1107612404460715,\n",
              " 0.7610456165918774,\n",
              " 0.7424289213950495,\n",
              " 0.17372936932575364,\n",
              " 0.7017587438619006,\n",
              " 0.09147273168996128,\n",
              " 0.7311938673418582,\n",
              " 0.2876547146440745,\n",
              " 0.04206626826720692,\n",
              " 0.08954608889244192,\n",
              " 0.10864468029534893,\n",
              " 0.17721965056674124,\n",
              " 0.2651954822280137,\n",
              " 0.04168908094398267,\n",
              " 0.6178350854961026,\n",
              " 0.8730863073679954,\n",
              " 0.29184603488340666,\n",
              " 0.03136540453257271,\n",
              " 0.1819746177649342,\n",
              " 0.20273381504168006,\n",
              " 0.40040000439650136,\n",
              " 0.11028826616064213,\n",
              " 0.1935396200507199,\n",
              " 0.09921972690170483,\n",
              " 0.1298061138288722,\n",
              " 0.6907151048260319,\n",
              " 0.24224595822669193,\n",
              " 0.07344587169700469,\n",
              " 0.7723765811853845,\n",
              " 0.4453543719267258,\n",
              " 0.18319650762113665,\n",
              " 0.15476882783618265,\n",
              " 0.538196202868456,\n",
              " 0.42441532078972405,\n",
              " 0.19712769388785312,\n",
              " 0.04415136819286863,\n",
              " 0.04548878036541459,\n",
              " 0.1268162880047115,\n",
              " 0.25143431497006297,\n",
              " 0.056052622057426435,\n",
              " 0.06370090323942315,\n",
              " 0.12295120573808103,\n",
              " 0.07222108171500487,\n",
              " 0.5962117002856266,\n",
              " 0.7550962159482884,\n",
              " 0.7156584033957523,\n",
              " 0.12467654218690528,\n",
              " 0.14903538771832603,\n",
              " 0.10646469104455618,\n",
              " 0.38328995766347207,\n",
              " 0.4181949469744881,\n",
              " 0.32603722229105137,\n",
              " 0.1829983910150707,\n",
              " 0.16804733286788695,\n",
              " 0.45337333324067186,\n",
              " 0.8348614172653688,\n",
              " 0.4259156275577618,\n",
              " 0.2463067611676036,\n",
              " 0.3812682154168409,\n",
              " 0.6476025974282271,\n",
              " 0.24712237488415506,\n",
              " 0.3014009158370573,\n",
              " 0.6977281703614674,\n",
              " 0.04447973626463422,\n",
              " 0.13847096611457385,\n",
              " 0.21503106571997677,\n",
              " 0.061344902537369704,\n",
              " 0.23523206598895438,\n",
              " 0.09223140503325931,\n",
              " 0.763775737734255,\n",
              " 0.7534911554034528,\n",
              " 0.18705389332734182,\n",
              " 0.35219804014817385,\n",
              " 0.04374329835212933,\n",
              " 0.1432638853681202,\n",
              " 0.09582984999177827,\n",
              " 0.2389832697078812,\n",
              " 0.47226342133361543,\n",
              " 0.1178317434507759,\n",
              " 0.7512116949296622,\n",
              " 0.19439286983707185,\n",
              " 0.774176309077382,\n",
              " 0.25259863751782646,\n",
              " 0.7636180139578431,\n",
              " 0.14089289036748956,\n",
              " 0.08872748445391908,\n",
              " 0.16354198224738065,\n",
              " 0.746480334911471,\n",
              " 0.7287139234197054,\n",
              " 0.06447440807026979,\n",
              " 0.08192739657213431,\n",
              " 0.12255298172354498,\n",
              " 0.6747353299575616,\n",
              " 0.5947701015735217,\n",
              " 0.558255962147689,\n",
              " 0.7221984408961414,\n",
              " 0.09363909652419752,\n",
              " 0.455880794328601,\n",
              " 0.13163783607155474,\n",
              " 0.43964039779947467,\n",
              " 0.7584635699185891,\n",
              " 0.14529998582560255,\n",
              " 0.09014106009000325,\n",
              " 0.07657195501918729,\n",
              " 0.21284841227735365,\n",
              " 0.18003010757682725,\n",
              " 0.1604940658237154,\n",
              " 0.12013447495646078,\n",
              " 0.18688497741859628,\n",
              " 0.31093958396809407,\n",
              " 0.0951865331811393,\n",
              " 0.10949534800872628,\n",
              " 0.09369728634592127,\n",
              " 0.14647001100942578,\n",
              " 0.07151791993093638,\n",
              " 0.17825947750103308,\n",
              " 0.11113399713961697,\n",
              " 0.7594249037998104,\n",
              " 0.1258900365831002,\n",
              " 0.1962499390178112,\n",
              " 0.396368675968439,\n",
              " 0.4870161087467754,\n",
              " 0.06533472510320786,\n",
              " 0.06000149090133045,\n",
              " 0.7068220259064631,\n",
              " 0.15904173773797883,\n",
              " 0.7298575184691782,\n",
              " 0.8206049247969798,\n",
              " 0.2883749978412615,\n",
              " 0.23711559845830982,\n",
              " 0.18026131701937803,\n",
              " 0.22467000127874223,\n",
              " 0.06938815932254687,\n",
              " 0.058497484163777216,\n",
              " 0.118160818718214,\n",
              " 0.09059392449397162,\n",
              " 0.6936556653574079,\n",
              " 0.14461062719215617,\n",
              " 0.06744898694833248,\n",
              " 0.09541149847858675,\n",
              " 0.10769791063197276,\n",
              " 0.10268054219596433,\n",
              " 0.39699672600094416,\n",
              " 0.7318825260367824,\n",
              " 0.5000325565947605,\n",
              " 0.04701249448683995,\n",
              " 0.06384831422009432,\n",
              " 0.11727803854842153,\n",
              " 0.3338396936760372,\n",
              " 0.6318715818828476,\n",
              " 0.836634938717869,\n",
              " 0.7390006692158192,\n",
              " 0.22096415837640843,\n",
              " 0.7814290941743309,\n",
              " 0.7772060999090398,\n",
              " 0.1168346884404991,\n",
              " 0.11882412747889763,\n",
              " 0.1113158469853136,\n",
              " 0.467223977096141,\n",
              " 0.5777712440781346,\n",
              " 0.37982772785221947,\n",
              " 0.12066592972382569,\n",
              " 0.140867071781261,\n",
              " 0.17963333974101342,\n",
              " 0.7616194254024211,\n",
              " 0.3361780091032486,\n",
              " 0.474170469434516,\n",
              " 0.24075812774894956,\n",
              " 0.12247246250737467,\n",
              " 0.09872231890088567,\n",
              " 0.336771100953294,\n",
              " 0.1347176627516399,\n",
              " 0.1633085701682307,\n",
              " 0.10915111839212804,\n",
              " 0.15060989177143652,\n",
              " 0.29139363778715605,\n",
              " 0.1152336722143892,\n",
              " 0.13136422364457162,\n",
              " 0.8044347518145484,\n",
              " 0.042331857728159485,\n",
              " 0.5415936540921031,\n",
              " 0.6735743940492457,\n",
              " 0.7301379546676233,\n",
              " 0.754889833422737,\n",
              " 0.069213409802612,\n",
              " 0.08012831984571574,\n",
              " 0.40683645603987306,\n",
              " 0.6764210333682404,\n",
              " 0.3626898397598963,\n",
              " 0.7310216335457288,\n",
              " 0.05414921984337138,\n",
              " 0.1381818875871897,\n",
              " 0.4632328513334999,\n",
              " 0.24311738152827284,\n",
              " 0.14521224576776962,\n",
              " 0.3100265212756173,\n",
              " 0.33120211968986996,\n",
              " 0.6931677533948108,\n",
              " 0.20245306393828086,\n",
              " 0.8165365308766501,\n",
              " 0.30213477971326463,\n",
              " 0.11526943184821226,\n",
              " 0.06696216921213342,\n",
              " 0.2612444625124382,\n",
              " 0.05650792828099279,\n",
              " 0.04799731666287246,\n",
              " 0.5719471904532583,\n",
              " 0.16471996365121255,\n",
              " 0.0760052355210853,\n",
              " 0.18593209248205886,\n",
              " 0.1145760298074197,\n",
              " 0.6934729704769971,\n",
              " 0.7712264679303542,\n",
              " 0.6671342438604181,\n",
              " 0.34928018777489916,\n",
              " 0.7360285502085304,\n",
              " 0.7242399539295565,\n",
              " 0.48751354862573026,\n",
              " 0.039125971777354523,\n",
              " 0.20599992876803352,\n",
              " 0.44960470366715155,\n",
              " 0.3032932040462587,\n",
              " 0.5399209255898142,\n",
              " 0.2137117562251869,\n",
              " 0.07850587763333337,\n",
              " 0.43468524718966894,\n",
              " 0.21772312555111473,\n",
              " 0.06970464329740088,\n",
              " 0.10061029393133887,\n",
              " 0.6962503704568128,\n",
              " 0.11417002484786462,\n",
              " 0.0545197705624335,\n",
              " 0.736367405114962,\n",
              " 0.06556397472688597,\n",
              " 0.7227852328113409,\n",
              " 0.1057425699141471,\n",
              " 0.6941870400439996,\n",
              " 0.7631480201450241,\n",
              " 0.1023879990736921,\n",
              " 0.14401324420801007,\n",
              " 0.7003572587312593,\n",
              " 0.06759196702474535,\n",
              " 0.5731575077996992,\n",
              " 0.8296405742980419,\n",
              " 0.09541241354329681,\n",
              " 0.17559149480613023,\n",
              " 0.23912047150475185,\n",
              " 0.6751001116687688,\n",
              " 0.3995900157476648,\n",
              " 0.7532632799710108,\n",
              " 0.6839857989252851,\n",
              " 0.16538368167037865,\n",
              " 0.18057410848509428,\n",
              " 0.1932312361954537,\n",
              " 0.11363802296778476,\n",
              " 0.12277814838247908,\n",
              " 0.16905788175952036,\n",
              " 0.4222618627336036,\n",
              " 0.7598959119076096,\n",
              " 0.36150205547887915,\n",
              " 0.39923258260161437,\n",
              " 0.06339453359171768,\n",
              " 0.7534617998036216,\n",
              " 0.19309686819443148,\n",
              " 0.060552921601093476,\n",
              " 0.13069377960816647,\n",
              " 0.7264150452233387,\n",
              " 0.6486967583688308,\n",
              " 0.687367839202975,\n",
              " 0.18287215252027858,\n",
              " 0.40221148333490786,\n",
              " 0.6074289362543699,\n",
              " 0.11687966945891377,\n",
              " 0.13513573806738316,\n",
              " 0.6589112210872778,\n",
              " 0.765957448416209,\n",
              " 0.22602651458438586,\n",
              " 0.6850255137817544,\n",
              " 0.15567610162585677,\n",
              " 0.1415243247265196,\n",
              " 0.11978947242170497,\n",
              " 0.08450672986061246,\n",
              " 0.3548979360577927,\n",
              " 0.11520970699953081,\n",
              " 0.048203005123576004,\n",
              " 0.12266284000968158,\n",
              " 0.8357386887813792,\n",
              " 0.040011261634018164,\n",
              " 0.35771877338821534,\n",
              " 0.13465922405714376,\n",
              " 0.06137585232266167,\n",
              " 0.1376651652451244,\n",
              " 0.6555846572495234,\n",
              " 0.1429322843518077,\n",
              " 0.09526964434847915,\n",
              " 0.1503187019371336,\n",
              " 0.28292129748419304]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFa0U-TbK0lm"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    }
  ]
}